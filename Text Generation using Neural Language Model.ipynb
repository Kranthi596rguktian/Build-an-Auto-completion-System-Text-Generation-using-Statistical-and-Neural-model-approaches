{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w2p_bqZM3WYu"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. Import Libraries\n",
    "2. Load Dataset\n",
    "3. Preprocessing and Exploring Text Data\n",
    "  \n",
    "  3.1 Text Cleaning\n",
    "  \n",
    "  3.2 Finding Word Count\n",
    "\n",
    "  3.3 Find and Replace Rare Words with \"Unknown\" Token\n",
    "\n",
    "4. Data Preparation\n",
    "\n",
    "  4.1 Prepare Sequences\n",
    "\n",
    "  4.2 Create Token-Integer Mappings\n",
    "\n",
    "  4.3 Split Data into Train and Validation Sets\n",
    "\n",
    "  4.4 Pad Sequences\n",
    "\n",
    "  4.5 Convert Text Sequences to Integer Sequences\n",
    "5. Model Building\n",
    "\n",
    "  5.1 Define Model Architecture\n",
    "  \n",
    "  5.2 Start Model Training\n",
    "6. Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZoPOnsX8uPS"
   },
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4693,
     "status": "ok",
     "timestamp": 1596871317154,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "Z_CCxOEI4iZK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "## Pickle library is used to read the pickle type of files\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4339,
     "status": "ok",
     "timestamp": 1596871317159,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "ypl18CrLFmt6",
    "outputId": "b4199d46-a7a3-4e1f-f2d5-94fac4bac47c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b714c2b5b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reproducing same results\n",
    "SEED = 2019\n",
    "\n",
    "# torch\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_5gPvXxWjru"
   },
   "source": [
    "# 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3300,
     "status": "ok",
     "timestamp": 1596871317161,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "RTcAv8MwCUep"
   },
   "outputs": [],
   "source": [
    "# open text file and read in data\n",
    "with open(\"dialogs_dataset\", \"rb\") as f:\n",
    "  dialogs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OqP9ohFb3-fM"
   },
   "source": [
    "Note: This is a subset of the Taskmaster dataset available under the Creative Commons Attribution 4.0 License. A full copy of the license can be found at https://creativecommons.org/licenses/by/4.0/. You can access the full dataset from [here](https://github.com/google-research-datasets/Taskmaster/tree/master/TM-1-2019).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 920,
     "status": "ok",
     "timestamp": 1596871317163,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "81_SXWZlE6Zb",
    "outputId": "eff429cf-724e-4653-cdda-46068a48035f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64776"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of text sequences\n",
    "len(dialogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 821,
     "status": "ok",
     "timestamp": 1596871317886,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "zSyAzbttAqP2",
    "outputId": "7b8a980c-cfda-4327-eaaa-98f431ad4249"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Oh my gosh, you're right\",\n",
       " \"That's exactly right\",\n",
       " 'No that will be all',\n",
       " 'Elder Theater, Jackson Center Ohio',\n",
       " 'Awesome, thanks so much',\n",
       " 'Hey man whatâ€™s up?',\n",
       " 'Probably Michael Jai White',\n",
       " 'Thank you so much Assistant',\n",
       " 'How will I recieve my tickets ',\n",
       " \"I'd like to find a nice restaurant for dinner\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print 10 random dialogs\n",
    "random.sample(dialogs, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGGTGRUDW9I3"
   },
   "source": [
    "# 3. Preprocessing and Exploring Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crvUl_ngM8sb"
   },
   "source": [
    "## 3.1 Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1535,
     "status": "ok",
     "timestamp": 1596871320701,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "kUuUKZgUFhkl"
   },
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "dialogs_clean = []\n",
    "\n",
    "for i in dialogs:\n",
    "  # remove everything except alphabets\n",
    "  i = re.sub(\"[^a-zA-Z' ]\", \"\", i)\n",
    "  # convert text to lowercase\n",
    "  i = i.lower()\n",
    "  # add cleaned text to the list\n",
    "  dialogs_clean.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1596871320704,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "DgZwFK1eSjkN",
    "outputId": "751d4097-be01-43a7-d25f-3466b7fbfe6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['okay thank you',\n",
       " 'the largest one they hae',\n",
       " 'no thank you',\n",
       " ' thank you for your help',\n",
       " 'yeah i like that idea that sounds perfect',\n",
       " 'hello do you think you can order me two tickets for shazam',\n",
       " 'ok thank you',\n",
       " 'yes i would also like to add an order of cinnasquares',\n",
       " 'hello i need an urgent appointment for my car at intelligent auto solutions',\n",
       " 'do they offer parking']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(dialogs_clean, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYDzFQVvNA7_"
   },
   "source": [
    "\n",
    "## 3.2 Finding Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1294,
     "status": "ok",
     "timestamp": 1596871322317,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "V1hrCYRp11UI"
   },
   "outputs": [],
   "source": [
    "# get list of all the words\n",
    "all_words = \" \".join(dialogs_clean).split()\n",
    "\n",
    "words_dict = {}\n",
    "\n",
    "# add word-count pair to the dictionary\n",
    "for word in all_words:   \n",
    "  # check if the word is already in dictionary \n",
    "  if word in words_dict:\n",
    "    # increment count of word by 1 \n",
    "    words_dict[word] = words_dict[word] + 1\n",
    "  else:\n",
    "    # add the word to dictionary with count 1 \n",
    "    words_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1219,
     "status": "ok",
     "timestamp": 1596871322775,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "gNxSGPubWaqA"
   },
   "outputs": [],
   "source": [
    "# prepare a dataframe\n",
    "words_df = pd.DataFrame({'word':list(words_dict.keys()), 'count':list(words_dict.values())})\n",
    "\n",
    "# sort words by their count in increasing order\n",
    "words_df = words_df.sort_values(by = ['count'])\n",
    "\n",
    "# reset dataframe index\n",
    "words_df.reset_index(inplace = True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1128,
     "status": "ok",
     "timestamp": 1596871323200,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "KVPbPsSWo-Ak",
    "outputId": "ab5dbe60-c9fe-4ed4-c09e-d44c121f78f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11147"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary size\n",
    "len(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1298,
     "status": "ok",
     "timestamp": 1596871323813,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "JTwmmOiEXBHt",
    "outputId": "fc5963db-838a-4f5d-ec13-846db582797a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uppermiddle</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shoots</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>geesh</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>andrea</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>precice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  count\n",
       "0  uppermiddle      1\n",
       "1       shoots      1\n",
       "2        geesh      1\n",
       "3       andrea      1\n",
       "4      precice      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 816,
     "status": "ok",
     "timestamp": 1596871323815,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "EWJEto8TMPiq",
    "outputId": "a9deb629-e282-46e3-e99a-e2094b3473e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11142</th>\n",
       "      <td>you</td>\n",
       "      <td>11909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11143</th>\n",
       "      <td>a</td>\n",
       "      <td>13380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11144</th>\n",
       "      <td>to</td>\n",
       "      <td>14000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11145</th>\n",
       "      <td>the</td>\n",
       "      <td>15406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11146</th>\n",
       "      <td>i</td>\n",
       "      <td>19654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  count\n",
       "11142  you  11909\n",
       "11143    a  13380\n",
       "11144   to  14000\n",
       "11145  the  15406\n",
       "11146    i  19654"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       uppermiddle\n",
       "1            shoots\n",
       "2             geesh\n",
       "3            andrea\n",
       "4           precice\n",
       "           ...     \n",
       "7690        corkage\n",
       "7691         become\n",
       "7692        pickups\n",
       "7693          clerk\n",
       "7694           calm\n",
       "Name: word, Length: 7695, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df[words_df['count'] < 4]['word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nTcWd-pYFRob"
   },
   "source": [
    "## 3.3 Find and Replace Rare Words with \"Unknown\" Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1894,
     "status": "ok",
     "timestamp": 1596871325920,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "1iC4ztG3XIP3"
   },
   "outputs": [],
   "source": [
    "# user specified threshold value\n",
    "rare_thresh = 4\n",
    "\n",
    "# get percentage of rare words in the vocabulary\n",
    "rare_words_count = len(words_df[words_df['count'] < rare_thresh]['word'])\n",
    "total_words = len(words_df) \n",
    "rare_dist = rare_words_count / total_words\n",
    "\n",
    "# coverage percentage of rare words in the corpus\n",
    "rare_cover = words_df[words_df['count'] < rare_thresh]['count'].sum()/words_df['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1102,
     "status": "ok",
     "timestamp": 1596871325922,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "helYHQ4BXNK9",
    "outputId": "71e44117-e1e5-4127-c876-8e75e3390988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rare words distribution in the vocabulary: 69.03\n",
      "Rare words coverage in the corpus: 2.27\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rare words distribution in the vocabulary: {rare_dist*100:.2f}\")\n",
    "print(f\"Rare words coverage in the corpus: {rare_cover*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1397,
     "status": "ok",
     "timestamp": 1596871326714,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "yJhbRQllXQJk"
   },
   "outputs": [],
   "source": [
    "# extract rare words in a list\n",
    "rare_words = words_df[words_df['count'] < rare_thresh]['word'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "178uBVztqTDa"
   },
   "source": [
    "Let's see the technique that we will use to replace the rare words/tokens in the dataset with a special token known as the unknown token (\"\\<unk\\>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 847,
     "status": "ok",
     "timestamp": 1596871327243,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "H5tSChzyoOJT",
    "outputId": "a853f457-5d0e-4878-c7eb-535c98192057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day|book|\n"
     ]
    }
   ],
   "source": [
    "## example\n",
    "# specify rare words\n",
    "r_words = [\"day\", \"book\"]\n",
    "\n",
    "# build pattern\n",
    "pattern = \"\"\n",
    "for i in r_words:\n",
    "  pattern+= \"{}|\".format(i)\n",
    "\n",
    "print(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1144,
     "status": "ok",
     "timestamp": 1596871328066,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "xZAMqmwPowZj",
    "outputId": "1401f1ef-5ff7-4e43-f819-0e6e7a4f289d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day|book\n"
     ]
    }
   ],
   "source": [
    "# removing the last element which is \"|\"\n",
    "pattern = pattern[:-1]\n",
    "print(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1214,
     "status": "ok",
     "timestamp": 1596871328646,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "UV1K1ibbo3XI",
    "outputId": "80949d89-1bc1-408d-f4b7-dc65a998f875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it has been a long  <unk> \n",
      "this  <unk>  is a must read\n"
     ]
    }
   ],
   "source": [
    "# replace the rare words with the <unk> token\n",
    "sents = [\"it has been a long day\", \"this book is a must read\"]\n",
    "\n",
    "for d in sents:\n",
    "  text = re.sub(pattern, \" <unk> \", d)\n",
    "  print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138,
     "referenced_widgets": [
      "92d7ebd42a4e463eaed43f3960cfe36c",
      "1eccf91efb3641ab93437844fb3d8515",
      "b00386afebe447d989c1d40fc75ebb24",
      "bb3bafbb85f14bd0a9904c4f08d40946",
      "433348a8133446d0a9a4dc38f028bdc6",
      "c684ec2cc5e9413a822c072387aeedd5",
      "233b3deec9394d288ed235c2054abe08",
      "cbb68ce201524cfa8eafec85788f9c42"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19783,
     "status": "ok",
     "timestamp": 1596871347751,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "hEn-L1_8YBjl",
    "outputId": "a0977257-41ce-4fc7-eb66-1190f88e5b00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-846b16966f4e>:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for d in tqdm_notebook(dialogs_clean):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321b673cd94c48dea799e1bf33e902df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=64776.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create a text pattern from the rare words, like \"word1 | word2 | word3...\"\n",
    "pattern = \"\"\n",
    "for i in rare_words:\n",
    "  pattern+= \" {} |\".format(i)\n",
    "\n",
    "# removing the last element which is \"|\"\n",
    "pattern = pattern[:-1]\n",
    "\n",
    "# empty list \n",
    "dialogs_clean_v2 = []\n",
    "\n",
    "# replace the rare words with the <unk> token\n",
    "for d in tqdm_notebook(dialogs_clean):\n",
    "  text = re.sub(pattern, \" <unk> \", d)\n",
    "  dialogs_clean_v2.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19223,
     "status": "ok",
     "timestamp": 1596871347752,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "SoQwO5FAZSP1",
    "outputId": "8caaf8a6-b6dd-4046-c1c0-80420e4dbaa8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['does it serve traditional chinese dessert',\n",
       " 'how much extra time to reach <unk> ',\n",
       " 'ok lets reserve a table for dinner at hakkasan',\n",
       " 'hello i need to get a car please',\n",
       " 'holiday inn <unk> parkconv <unk> convention center drive <unk> park il',\n",
       " 'bowling alley <unk> highway <unk> park il',\n",
       " 'what types of cars does uber have',\n",
       " \"what's the price difference\",\n",
       " 'ok get me the cheapest please',\n",
       " 'ok then get me the next level']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogs_clean_v2[520:530]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "waTukoy1AbVT"
   },
   "source": [
    "# 4. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8SvIJQ0brBWW"
   },
   "source": [
    "## 4.1 Prepare Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17411,
     "status": "ok",
     "timestamp": 1596871347754,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "H5BYGcwBF7hX",
    "outputId": "2559acd7-fec8-4b47-f91f-40a3857f0637"
   },
   "outputs": [],
   "source": [
    "# capture length of all the sequences\n",
    "text_word_count = []\n",
    "for i in dialogs_clean_v2:\n",
    "  text_word_count.append(len(i.split()))\n",
    "        \n",
    "# plot the sequence lengths\n",
    "pd.Series(text_word_count).hist(bins = 30,range=(0,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16835,
     "status": "ok",
     "timestamp": 1596871347755,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "uw8F6bfwjBeo"
   },
   "outputs": [],
   "source": [
    "# function to create sequences of equal length\n",
    "def create_seq(text, seq_len = 5):\n",
    "         \n",
    "  sequences = []    \n",
    "  \n",
    "  if len(text.split()) > seq_len:\n",
    "    for i in range(seq_len, len(text.split())):\n",
    "      # select sequence of tokens\n",
    "      seq = text.split()[i-seq_len:i+1]\n",
    "      # append sequence to the list\n",
    "      sequences.append(\" \".join(seq))\n",
    "\n",
    "    return sequences\n",
    "\n",
    "  else:\n",
    "    \n",
    "    return [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16794,
     "status": "ok",
     "timestamp": 1596871348315,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "p0psJCgo9QtH"
   },
   "outputs": [],
   "source": [
    "# create sequences of equal length\n",
    "seqs = [create_seq(i) for i in dialogs_clean_v2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16070,
     "status": "ok",
     "timestamp": 1596871348316,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "LCrf5t5vCI6I",
    "outputId": "8c4b8d92-3128-467d-9c60-eab93af446d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"hi i'm looking to book a\",\n",
       "  \"i'm looking to book a table\",\n",
       "  'looking to book a table for',\n",
       "  'to book a table for korean',\n",
       "  'book a table for korean fod'],\n",
       " ['somewhere in southern nyc maybe the',\n",
       "  'in southern nyc maybe the east',\n",
       "  'southern nyc maybe the east village'],\n",
       " [\"we don't want to sit at\",\n",
       "  \"don't want to sit at the\",\n",
       "  'want to sit at the bar',\n",
       "  'to sit at the bar but',\n",
       "  'sit at the bar but anywhere',\n",
       "  'at the bar but anywhere else',\n",
       "  'the bar but anywhere else is',\n",
       "  'bar but anywhere else is fine'],\n",
       " ['what times are available'],\n",
       " [\"yikes we can't do those times\"],\n",
       " ['let me check'],\n",
       " [\"great let's book that\"],\n",
       " [\"no that's it just book\"],\n",
       " ['hi i would like to see',\n",
       "  'i would like to see if',\n",
       "  'would like to see if the',\n",
       "  'like to see if the movie',\n",
       "  'to see if the movie what',\n",
       "  'see if the movie what men',\n",
       "  'if the movie what men want',\n",
       "  'the movie what men want is',\n",
       "  'movie what men want is playing',\n",
       "  'what men want is playing here'],\n",
       " ['yes for me and a friend',\n",
       "  'for me and a friend so',\n",
       "  'me and a friend so two',\n",
       "  'and a friend so two tickets',\n",
       "  'a friend so two tickets please']]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 65541,
     "status": "ok",
     "timestamp": 1596871398538,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "S4O3-S6PA78w"
   },
   "outputs": [],
   "source": [
    "# merge list-of-lists into a single list\n",
    "seqs = sum(seqs, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 64651,
     "status": "ok",
     "timestamp": 1596871398543,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "dwmBsxuqxPRq",
    "outputId": "35a8e7c5-df33-411d-cb30-4c011f685d10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hi i'm looking to book a\",\n",
       " \"i'm looking to book a table\",\n",
       " 'looking to book a table for',\n",
       " 'to book a table for korean',\n",
       " 'book a table for korean fod',\n",
       " 'somewhere in southern nyc maybe the',\n",
       " 'in southern nyc maybe the east',\n",
       " 'southern nyc maybe the east village',\n",
       " \"we don't want to sit at\",\n",
       " \"don't want to sit at the\",\n",
       " 'want to sit at the bar',\n",
       " 'to sit at the bar but',\n",
       " 'sit at the bar but anywhere',\n",
       " 'at the bar but anywhere else',\n",
       " 'the bar but anywhere else is']"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63767,
     "status": "ok",
     "timestamp": 1596871398546,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "cNW5MIpTDufa",
    "outputId": "311f64a5-6a15-47ec-cef7-521b49f62f5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205346"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count of sequences\n",
    "len(seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62945,
     "status": "ok",
     "timestamp": 1596871398548,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "M0dppnffaEgG"
   },
   "outputs": [],
   "source": [
    "# create input and target sequences (x and y)\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for s in seqs:\n",
    "  x.append(\" \".join(s.split()[:-1]))\n",
    "  y.append(\" \".join(s.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62137,
     "status": "ok",
     "timestamp": 1596871398550,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "27XKPYgLwuOF",
    "outputId": "995c0896-0844-4c50-f777-0a45651fc895"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"hi i'm looking to book\", \"i'm looking to book a\")"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 61130,
     "status": "ok",
     "timestamp": 1596871398552,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "rVge-c6WHUx1",
    "outputId": "e8de2d34-e100-4072-8f21-5bb9e96fe908"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('to drive to several locations', 'drive to several locations do')"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[88543], y[88543]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IzR5aG9wrZJ5"
   },
   "source": [
    "## 4.2 Create Token-Integer Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 59361,
     "status": "ok",
     "timestamp": 1596871398555,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "CSgaezAI60ht"
   },
   "outputs": [],
   "source": [
    "# create integer-to-token mapping\n",
    "int2token = {}\n",
    "cnt = 1\n",
    "\n",
    "for w in set(\" \".join(dialogs_clean_v2).split()):\n",
    "  int2token[cnt] = w\n",
    "  cnt+= 1\n",
    "\n",
    "# create token-to-integer mapping\n",
    "token2int = {t: i for i, t in int2token.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 58590,
     "status": "ok",
     "timestamp": 1596871398558,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "j5TSElOk_OQf",
    "outputId": "3eda0c7c-51e1-4792-d826-b441f5ad621c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3170, 'exhausted')"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2int[\"can\"], int2token[1127]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQOk80iBr51V"
   },
   "source": [
    "## 4.3 Split Data into Train and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56875,
     "status": "ok",
     "timestamp": 1596871398559,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "Jn6a1aXvAcRl"
   },
   "outputs": [],
   "source": [
    "# train-validation split\n",
    "# input sequences\n",
    "x_tr = x[:150000]\n",
    "x_val = x[150000:]\n",
    "\n",
    "# target sequences\n",
    "y_tr = y[:150000]\n",
    "y_val = y[150000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YIfKsjE3sxgu"
   },
   "source": [
    "## 4.4 Pad Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55912,
     "status": "ok",
     "timestamp": 1596871399525,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "QlG-sd79sNjH",
    "outputId": "912091b4-6c04-435c-efc0-af15733d22ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1a3bd3f2e8>"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot sequence length in train set\n",
    "text_word_count = []\n",
    "\n",
    "for i in x_tr:\n",
    "  text_word_count.append(len(i.split()))\n",
    "\n",
    "pd.Series(text_word_count).hist(bins = 70,range=(0,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 55185,
     "status": "ok",
     "timestamp": 1596871399528,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "2umo7Mns5NKl"
   },
   "outputs": [],
   "source": [
    "# based on the plot above\n",
    "max_text_len = 5\n",
    "\n",
    "# function to perform padding\n",
    "def pad_sequence(seq, n):\n",
    "\n",
    "  # split input sequence into tokens\n",
    "  seq = seq.split()\n",
    "  \n",
    "  # check if no. of tokens in input sequence is less than 'n'\n",
    "  if len(seq) < n:\n",
    "    for i in range(n - len(seq)):\n",
    "      seq.append(\"<pad>\")\n",
    "\n",
    "  return \" \".join(seq)\n",
    "\n",
    "# pad text sequences (train set)\n",
    "x_tr_padded = [pad_sequence(s, max_text_len) for s in x_tr]\n",
    "y_tr_padded = [pad_sequence(s, max_text_len) for s in y_tr]\n",
    "\n",
    "# pad text sequences (validation set)\n",
    "x_val_padded = [pad_sequence(s, max_text_len) for s in x_val]\n",
    "y_val_padded = [pad_sequence(s, max_text_len) for s in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 54370,
     "status": "ok",
     "timestamp": 1596871399530,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "gxwn3NyD-HBD",
    "outputId": "a42ed43e-ae07-43af-abc0-8cd67799a379"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hi i'm looking to book\",\n",
       " \"i'm looking to book a\",\n",
       " 'looking to book a table',\n",
       " 'to book a table for',\n",
       " 'book a table for korean',\n",
       " 'somewhere in southern nyc maybe',\n",
       " 'in southern nyc maybe the',\n",
       " 'southern nyc maybe the east',\n",
       " \"we don't want to sit\",\n",
       " \"don't want to sit at\",\n",
       " 'want to sit at the',\n",
       " 'to sit at the bar',\n",
       " 'sit at the bar but',\n",
       " 'at the bar but anywhere',\n",
       " 'the bar but anywhere else',\n",
       " 'bar but anywhere else is',\n",
       " 'what times are <pad> <pad>',\n",
       " \"yikes we can't do those\",\n",
       " 'let me <pad> <pad> <pad>',\n",
       " \"great let's book <pad> <pad>\"]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_padded[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53577,
     "status": "ok",
     "timestamp": 1596871399532,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "PPEGI_Xf-ne_",
    "outputId": "337f39bb-63e0-4660-ce3b-d1c873a023d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"i'm looking to book a\",\n",
       " 'looking to book a table',\n",
       " 'to book a table for',\n",
       " 'book a table for korean',\n",
       " 'a table for korean fod',\n",
       " 'in southern nyc maybe the',\n",
       " 'southern nyc maybe the east',\n",
       " 'nyc maybe the east village',\n",
       " \"don't want to sit at\",\n",
       " 'want to sit at the',\n",
       " 'to sit at the bar',\n",
       " 'sit at the bar but',\n",
       " 'at the bar but anywhere',\n",
       " 'the bar but anywhere else',\n",
       " 'bar but anywhere else is',\n",
       " 'but anywhere else is fine',\n",
       " 'times are available <pad> <pad>',\n",
       " \"we can't do those times\",\n",
       " 'me check <pad> <pad> <pad>',\n",
       " \"let's book that <pad> <pad>\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_padded[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52801,
     "status": "ok",
     "timestamp": 1596871399533,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "mlUnd9v_-4ad"
   },
   "outputs": [],
   "source": [
    "# update mapping dictionaries\n",
    "int2token[0] = \"<pad>\"\n",
    "token2int[\"<pad>\"] = 0\n",
    "\n",
    "# set vocabulary size\n",
    "vocab_size = len(int2token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLqEIRavtMpY"
   },
   "source": [
    "## 4.5 Convert Text Sequences to Integer Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51264,
     "status": "ok",
     "timestamp": 1596871399535,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "HVxwznePACXC"
   },
   "outputs": [],
   "source": [
    "# function to create integer sequences\n",
    "def get_integer_seq(seq):\n",
    "  return [token2int[w] for w in seq.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51299,
     "status": "ok",
     "timestamp": 1596871400387,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "r7QVnobNARnr"
   },
   "outputs": [],
   "source": [
    "# convert text sequences to integer sequences\n",
    "x_tr_int = [get_integer_seq(i) for i in x_tr_padded]\n",
    "y_tr_int = [get_integer_seq(i) for i in y_tr_padded]\n",
    "\n",
    "x_val_int = [get_integer_seq(i) for i in x_val_padded]\n",
    "y_val_int = [get_integer_seq(i) for i in y_val_padded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50390,
     "status": "ok",
     "timestamp": 1596871400388,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "qo8g1L_FwAOC",
    "outputId": "790f6769-2037-47b1-dda4-0b2787774789"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2478, 5856, 731, 5673, 2249],\n",
       " [5856, 731, 5673, 2249, 4709],\n",
       " [731, 5673, 2249, 4709, 3575],\n",
       " [5673, 2249, 4709, 3575, 4899],\n",
       " [2249, 4709, 3575, 4899, 3888],\n",
       " [3817, 1075, 928, 5000, 6379],\n",
       " [1075, 928, 5000, 6379, 3508],\n",
       " [928, 5000, 6379, 3508, 3603],\n",
       " [371, 6239, 6027, 5673, 3634],\n",
       " [6239, 6027, 5673, 3634, 4954]]"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr_int[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 49598,
     "status": "ok",
     "timestamp": 1596871400390,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "yGa_ORxZwDkw",
    "outputId": "130c0343-d1f4-49c0-816d-796c3eccd77b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5856, 731, 5673, 2249, 4709],\n",
       " [731, 5673, 2249, 4709, 3575],\n",
       " [5673, 2249, 4709, 3575, 4899],\n",
       " [2249, 4709, 3575, 4899, 3888],\n",
       " [4709, 3575, 4899, 3888, 2413],\n",
       " [1075, 928, 5000, 6379, 3508],\n",
       " [928, 5000, 6379, 3508, 3603],\n",
       " [5000, 6379, 3508, 3603, 2330],\n",
       " [6239, 6027, 5673, 3634, 4954],\n",
       " [6027, 5673, 3634, 4954, 3508]]"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_int[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48717,
     "status": "ok",
     "timestamp": 1596871400391,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "eLntLMs5NwGi",
    "outputId": "a11c9e89-ab24-48b0-f616-5e06ad13aae0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150000, 5), (150000, 5), (55346, 5), (55346, 5))"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert lists into numpy arrays\n",
    "x_tr_int = np.array(x_tr_int)\n",
    "y_tr_int = np.array(y_tr_int)\n",
    "\n",
    "x_val_int = np.array(x_val_int)\n",
    "y_val_int = np.array(y_val_int)\n",
    "\n",
    "x_tr_int.shape, y_tr_int.shape, x_val_int.shape, y_val_int.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Y3bZcFywH33"
   },
   "source": [
    "# 5. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tCh2prnryopJ"
   },
   "source": [
    "## 5.1 Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38190,
     "status": "ok",
     "timestamp": 1596871400392,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "ldcidFnPRsOM"
   },
   "outputs": [],
   "source": [
    "# define model architecture\n",
    "'''Till now we converted the text sequences data into arrays of integer sequences. \n",
    "In this part, we will use these arrays to train our LSTM based language model.\n",
    "So let's define the architecture of the model first. \n",
    "1.The model will first take input integer sequences, which will be passed through the embedding layer and the output dimensions\n",
    "of the output of the embedding layer would be 200 that means, it would generate word embeddings or word vectors of length 200.\n",
    "and input dimensions for this layer is equal to the vocabulary size, which is equal to the count of distinct tokens in the training set.\n",
    "'''\n",
    "## embedding layer: \n",
    "##    input dim = vocab_size, \n",
    "##    ouput dim = 200\n",
    "'''Then the output of the embedding layer will be passed to the LSDM layer one time step at a time. \n",
    "As you can see over the LSTM architecture(imgine) at time step one, the first token is passed to the model,\n",
    "at time step two, the second token will be passed to the model and so on. \n",
    "And for each time step, the input dimensions are 200, and the output dimensions are 256.\n",
    "This number of output dimensions can be set by the user. So it can be 256 or it can be set to any other number such as 512 or 120.\n",
    "It's a hyper parameter. '''\n",
    "## LSTM layer:\n",
    "##    input dim = 200\n",
    "##    hidden units = 256\n",
    "##    layers = 2\n",
    "##    output dim = 256\n",
    "'''Then the output of this LSTM layer will be passed to a dropout layer, so there would be no change in the dimensions. \n",
    "'''\n",
    "## Dropout Layer\n",
    "##    input dim = 256\n",
    "##    output dim = 256\n",
    "'''And finally, the output of the dropout layer will be passed to a fully connected layer. \n",
    "So this fully connected layer would be applied at each time step, and the input dimensions would be 256,\n",
    "and the output dimensions would be a vocabulary size. '''\n",
    "## fully connected layer\n",
    "##    input dim = 256\n",
    "##    ouput dim = vocab_size\n",
    "\n",
    "'''Now let's see how we can define this architecture in Pytorch.'''\n",
    "'The model architecute would be define in the class WordLSTM class'\n",
    "class WordLSTM(nn.Module):\n",
    "      \n",
    "  def __init__(self, n_hidden=256, n_layers=2, drop_prob=0.3, lr=0.001):\n",
    "    super().__init__()\n",
    "    self.drop_prob = drop_prob\n",
    "    self.n_layers = n_layers\n",
    "    self.n_hidden = n_hidden\n",
    "    self.lr = lr\n",
    "    \n",
    "    self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "\n",
    "    ## define the LSTM\n",
    "    # input data is of shape (batch size, sequence length, no. of features) 3dimentional...\n",
    "    # ...therefore we need batch_first=True or else by default LSTM layer considers the 1st dimenstion as the sequencd lenth\n",
    "    self.lstm = nn.LSTM(200, n_hidden, n_layers, batch_first=True)\n",
    "    \n",
    "    ## define a dropout layer\n",
    "    self.dropout = nn.Dropout(drop_prob)\n",
    "    \n",
    "    ## define the fully-connected layer\n",
    "    self.fc = nn.Linear(n_hidden, vocab_size)      \n",
    "  \n",
    "  def forward(self, x, hidden):\n",
    "    ''' Forward pass through the network. \n",
    "        These inputs are x, and the hidden/cell state is `hidden`. '''\n",
    "\n",
    "    ## pass input through embedding layer\n",
    "    embedded = self.emb_layer(x)     \n",
    "    \n",
    "    ## Get the outputs and the new hidden state from the lstm\n",
    "    ''' Since we are passing batches of  tensors as inputs, output of the embedding layer will be passed to the LSTM layer.\n",
    "    it will give two outputs: lstm_output, hidden\n",
    "    \"lstm_output\" is nothing but a tensors of hidden states from all the time steps.\n",
    "    \"hidden\" is the hidden state from the last time step.'''\n",
    "    \n",
    "    lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "    \n",
    "    ## pass through a dropout layer\n",
    "    out = self.dropout(lstm_output)\n",
    "    \n",
    "    ## reshape the tensor to the shape (batch-size*sequence length, hidden units)\n",
    "    out = out.reshape(-1, self.n_hidden)\n",
    "\n",
    "    ## put \"out\" through the fully-connected layer\n",
    "    out = self.fc(out)\n",
    "\n",
    "    # return the final output and the hidden state\n",
    "    return out, hidden\n",
    "    \n",
    "    \n",
    "  def init_hidden(self, batch_size):\n",
    "    ''' Initializes hidden state for time step one, here we are creating two tensors.\n",
    "    One is for the hidden state and the other is for the cell state because an LSTM cell\n",
    "    has two statess , cellstate and hiddenstate.'''\n",
    "    # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "    # initialized to zero, for hidden state and cell state of LSTM\n",
    "    weight = next(self.parameters()).data\n",
    "\n",
    "    if (torch.cuda.is_available()):\n",
    "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "    else:\n",
    "      hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "    \n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:  weight = next(self.parameters()).data and weight.new() **\n",
    "- Ans: self.parameters() is a generator method that iterates over the parameters of the model.So weight variable simply holds a parameter of the model.\n",
    "\n",
    "- Then weight.new() creates a tensor that has the same data type, same device as the produced parameter.\n",
    "\n",
    "- next retrieve the next item from the iterator by calling its next() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14294,
     "status": "ok",
     "timestamp": 1596871400394,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "LQq72w9ADLn_",
    "outputId": "bf5e2652-1433-40c0-d0f0-e07e7fe6e79d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordLSTM(\n",
      "  (emb_layer): Embedding(6502, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=6502, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "net = WordLSTM()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13266,
     "status": "ok",
     "timestamp": 1596871400396,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "2pRtYvAVFCKB"
   },
   "outputs": [],
   "source": [
    "'''We will be passing batches of tensors as input to the model so we need a function that would create\n",
    "or generate batches of both input sequence and target sequences.'''\n",
    "# function to generate batches\n",
    "def get_batches(arr_x, arr_y, batch_size):\n",
    "  # iterate through the arrays\n",
    "  prv = 0\n",
    "  \n",
    "  for n in range(batch_size, arr_x.shape[0], batch_size):\n",
    "    # batch of input sequences\n",
    "    x = arr_x[prv:n,:]\n",
    "\n",
    "    # batch of target sequences\n",
    "    y = arr_y[prv:n,:]\n",
    "\n",
    "    prv = n\n",
    "    \n",
    "    yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HpV3vbVeytW1"
   },
   "source": [
    "## 5.2 Start Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1132,
     "status": "ok",
     "timestamp": 1596871404096,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "QHMBy8HQ2GhY"
   },
   "outputs": [],
   "source": [
    "'''Till now we define our model, it is time to train it\n",
    "print_every=32 print the results after every 30 seconds pass to the model.'''\n",
    "\n",
    "def train(net, epochs=10, batch_size=32, lr=0.001, print_every=32):\n",
    "      \n",
    "  # set initial loss to infinite\n",
    "  best_valid_loss = float('inf')\n",
    "  \n",
    "  # optimizer \n",
    "  opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "  \n",
    "  # loss function: In pytorch the way cross entropy loss is defined is that it also\n",
    "  # applies softmax operation to it's input. Therefore, we did not use the softmax at the final\n",
    "  # at the final layer in the model architecture\n",
    "  \n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "  if(torch.cuda.is_available()):\n",
    "    # push model to GPU\n",
    "    net.cuda()\n",
    "  \n",
    "  counter = 0\n",
    "\n",
    "  net.train()\n",
    "\n",
    "  for e in range(epochs):\n",
    "            \n",
    "\n",
    "    # iterate over batches\n",
    "    for x, y in get_batches(x_tr_int, y_tr_int, batch_size):\n",
    "      counter+= 1\n",
    "      \n",
    "      # convert arrays to tensors\n",
    "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "      \n",
    "      if(torch.cuda.is_available()):\n",
    "        # push tensors to GPU\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "      # initialize hidden state\n",
    "      h = net.init_hidden(batch_size)\n",
    "      '''Note that every time before we pass a battch to model, the hidden state will be\n",
    "      initialized because otherwise it would carry some information from the previous sequences of the previous batch.\n",
    "      Ideally, we should initialize hidden state for each sequence in the training data.\n",
    "      But since we are training the model batch-wise.\n",
    "      The best that we can do is to initialize the hidden state at the start of each batch.'''\n",
    "\n",
    "      # set accumulated gradients to zero\n",
    "      net.zero_grad()\n",
    "      \n",
    "      # get the output from the model\n",
    "      '''output: the output at each time step of all the sequences in the batch.\n",
    "      h  : final hidden states of all the sequences in the batch.'''\n",
    "      output, h = net(inputs, h)\n",
    "      \n",
    "      # calculate the loss and perform backprop\n",
    "      loss = criterion(output, targets.view(-1))\n",
    "      loss.backward()\n",
    "      \n",
    "      opt.step()\n",
    "      \n",
    "      if counter % print_every == 0:\n",
    "        '''The default values of print_every is 32, so at every 32nd step, all the validation batches \n",
    "        will be passed to the model.'''\n",
    "        # Get validation loss\n",
    "        \n",
    "        val_losses = []\n",
    "\n",
    "        net.eval()\n",
    "        for x, y in get_batches(x_val_int, y_val_int, batch_size):\n",
    "            \n",
    "          x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "          # torch.from_numpy(x).long()\n",
    "          \n",
    "          val_h = net.init_hidden(batch_size)\n",
    "\n",
    "          inputs, targets = x, y\n",
    "          if(torch.cuda.is_available()):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "          output, val_h = net(inputs, val_h)\n",
    "\n",
    "          val_loss = criterion(output, targets.view(-1))\n",
    "          val_losses.append(val_loss.item())\n",
    "\n",
    "        #save the best model\n",
    "        if np.mean(val_losses) < best_valid_loss:\n",
    "          best_valid_loss = np.mean(val_losses)\n",
    "          torch.save(net.state_dict(), 'saved_weights.pt')\n",
    "\n",
    "        net.train()\n",
    "\n",
    "      \n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "              \"Step: {}...\".format(counter),\n",
    "              \"Loss: {:.4f}...\".format(loss.item()),\n",
    "              \"ppl: {:.4f} \".format(np.exp(np.mean(val_losses))),\n",
    "              \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 739047,
     "status": "ok",
     "timestamp": 1596872148847,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "sbGexKELaTNb",
    "outputId": "ddbf5b26-2644-4356-dcaa-f8a17dc72f40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 32... Loss: 6.4697... ppl: 696.1589  Val Loss: 6.5456\n",
      "Epoch: 1/10... Step: 64... Loss: 5.6610... ppl: 346.2154  Val Loss: 5.8471\n",
      "Epoch: 1/10... Step: 96... Loss: 6.0162... ppl: 269.5304  Val Loss: 5.5967\n",
      "Epoch: 1/10... Step: 128... Loss: 5.2267... ppl: 231.3166  Val Loss: 5.4438\n",
      "Epoch: 1/10... Step: 160... Loss: 6.1923... ppl: 204.2679  Val Loss: 5.3194\n",
      "Epoch: 1/10... Step: 192... Loss: 5.6782... ppl: 181.0216  Val Loss: 5.1986\n",
      "Epoch: 1/10... Step: 224... Loss: 5.1817... ppl: 164.4020  Val Loss: 5.1023\n",
      "Epoch: 1/10... Step: 256... Loss: 4.2399... ppl: 149.1423  Val Loss: 5.0049\n",
      "Epoch: 1/10... Step: 288... Loss: 4.9247... ppl: 137.4690  Val Loss: 4.9234\n",
      "Epoch: 1/10... Step: 320... Loss: 4.9262... ppl: 126.9546  Val Loss: 4.8438\n",
      "Epoch: 1/10... Step: 352... Loss: 4.3369... ppl: 118.3985  Val Loss: 4.7741\n",
      "Epoch: 1/10... Step: 384... Loss: 5.0263... ppl: 111.4259  Val Loss: 4.7134\n",
      "Epoch: 1/10... Step: 416... Loss: 4.8335... ppl: 106.5064  Val Loss: 4.6682\n",
      "Epoch: 1/10... Step: 448... Loss: 4.3957... ppl: 101.3758  Val Loss: 4.6188\n",
      "Epoch: 1/10... Step: 480... Loss: 4.2231... ppl: 97.2551  Val Loss: 4.5773\n",
      "Epoch: 1/10... Step: 512... Loss: 4.4387... ppl: 93.4499  Val Loss: 4.5374\n",
      "Epoch: 1/10... Step: 544... Loss: 4.7528... ppl: 90.1301  Val Loss: 4.5013\n",
      "Epoch: 1/10... Step: 576... Loss: 5.5581... ppl: 87.2691  Val Loss: 4.4690\n",
      "Epoch: 1/10... Step: 608... Loss: 4.4433... ppl: 85.1168  Val Loss: 4.4440\n",
      "Epoch: 1/10... Step: 640... Loss: 4.4443... ppl: 82.1874  Val Loss: 4.4090\n",
      "Epoch: 1/10... Step: 672... Loss: 4.3550... ppl: 80.5787  Val Loss: 4.3892\n",
      "Epoch: 1/10... Step: 704... Loss: 4.2699... ppl: 78.1624  Val Loss: 4.3588\n",
      "Epoch: 1/10... Step: 736... Loss: 4.7827... ppl: 76.8165  Val Loss: 4.3414\n",
      "Epoch: 1/10... Step: 768... Loss: 3.8664... ppl: 74.3597  Val Loss: 4.3089\n",
      "Epoch: 1/10... Step: 800... Loss: 4.2361... ppl: 72.7898  Val Loss: 4.2876\n",
      "Epoch: 1/10... Step: 832... Loss: 4.6800... ppl: 71.3758  Val Loss: 4.2680\n",
      "Epoch: 1/10... Step: 864... Loss: 4.0725... ppl: 70.2179  Val Loss: 4.2516\n",
      "Epoch: 1/10... Step: 896... Loss: 4.3213... ppl: 69.0626  Val Loss: 4.2350\n",
      "Epoch: 1/10... Step: 928... Loss: 4.3300... ppl: 67.2784  Val Loss: 4.2088\n",
      "Epoch: 1/10... Step: 960... Loss: 3.9601... ppl: 66.6369  Val Loss: 4.1993\n",
      "Epoch: 1/10... Step: 992... Loss: 3.8268... ppl: 65.1879  Val Loss: 4.1773\n",
      "Epoch: 1/10... Step: 1024... Loss: 3.6277... ppl: 64.0340  Val Loss: 4.1594\n",
      "Epoch: 1/10... Step: 1056... Loss: 4.2118... ppl: 63.0479  Val Loss: 4.1439\n",
      "Epoch: 1/10... Step: 1088... Loss: 3.4191... ppl: 62.1375  Val Loss: 4.1293\n",
      "Epoch: 1/10... Step: 1120... Loss: 4.1166... ppl: 61.4359  Val Loss: 4.1180\n",
      "Epoch: 1/10... Step: 1152... Loss: 3.2171... ppl: 60.5534  Val Loss: 4.1035\n",
      "Epoch: 1/10... Step: 1184... Loss: 3.9884... ppl: 59.7876  Val Loss: 4.0908\n",
      "Epoch: 1/10... Step: 1216... Loss: 3.7596... ppl: 59.0948  Val Loss: 4.0791\n",
      "Epoch: 1/10... Step: 1248... Loss: 4.1144... ppl: 58.1678  Val Loss: 4.0633\n",
      "Epoch: 1/10... Step: 1280... Loss: 4.2733... ppl: 57.3297  Val Loss: 4.0488\n",
      "Epoch: 1/10... Step: 1312... Loss: 4.8577... ppl: 57.2641  Val Loss: 4.0477\n",
      "Epoch: 1/10... Step: 1344... Loss: 3.8097... ppl: 56.4849  Val Loss: 4.0340\n",
      "Epoch: 1/10... Step: 1376... Loss: 4.1814... ppl: 55.4797  Val Loss: 4.0160\n",
      "Epoch: 1/10... Step: 1408... Loss: 3.5835... ppl: 54.9363  Val Loss: 4.0062\n",
      "Epoch: 1/10... Step: 1440... Loss: 4.1155... ppl: 54.5530  Val Loss: 3.9992\n",
      "Epoch: 1/10... Step: 1472... Loss: 3.4247... ppl: 53.9423  Val Loss: 3.9879\n",
      "Epoch: 1/10... Step: 1504... Loss: 4.1303... ppl: 53.7155  Val Loss: 3.9837\n",
      "Epoch: 1/10... Step: 1536... Loss: 4.8345... ppl: 53.3269  Val Loss: 3.9764\n",
      "Epoch: 1/10... Step: 1568... Loss: 4.0022... ppl: 52.1621  Val Loss: 3.9544\n",
      "Epoch: 1/10... Step: 1600... Loss: 3.7338... ppl: 51.8270  Val Loss: 3.9479\n",
      "Epoch: 1/10... Step: 1632... Loss: 5.0833... ppl: 51.0752  Val Loss: 3.9333\n",
      "Epoch: 1/10... Step: 1664... Loss: 4.0519... ppl: 50.7435  Val Loss: 3.9268\n",
      "Epoch: 1/10... Step: 1696... Loss: 3.6035... ppl: 50.5497  Val Loss: 3.9230\n",
      "Epoch: 1/10... Step: 1728... Loss: 3.6834... ppl: 49.9738  Val Loss: 3.9115\n",
      "Epoch: 1/10... Step: 1760... Loss: 4.4421... ppl: 49.9282  Val Loss: 3.9106\n",
      "Epoch: 1/10... Step: 1792... Loss: 3.4618... ppl: 49.2971  Val Loss: 3.8979\n",
      "Epoch: 1/10... Step: 1824... Loss: 4.3022... ppl: 49.5773  Val Loss: 3.9035\n",
      "Epoch: 1/10... Step: 1856... Loss: 5.6973... ppl: 48.5292  Val Loss: 3.8822\n",
      "Epoch: 1/10... Step: 1888... Loss: 3.6119... ppl: 47.7426  Val Loss: 3.8658\n",
      "Epoch: 1/10... Step: 1920... Loss: 3.4999... ppl: 47.5141  Val Loss: 3.8610\n",
      "Epoch: 1/10... Step: 1952... Loss: 3.0146... ppl: 47.1731  Val Loss: 3.8538\n",
      "Epoch: 1/10... Step: 1984... Loss: 2.9163... ppl: 47.1206  Val Loss: 3.8527\n",
      "Epoch: 1/10... Step: 2016... Loss: 3.5108... ppl: 46.5519  Val Loss: 3.8406\n",
      "Epoch: 1/10... Step: 2048... Loss: 4.0596... ppl: 46.3527  Val Loss: 3.8363\n",
      "Epoch: 1/10... Step: 2080... Loss: 3.7950... ppl: 46.0311  Val Loss: 3.8293\n",
      "Epoch: 1/10... Step: 2112... Loss: 4.7219... ppl: 45.9016  Val Loss: 3.8265\n",
      "Epoch: 1/10... Step: 2144... Loss: 3.4557... ppl: 45.6446  Val Loss: 3.8209\n",
      "Epoch: 1/10... Step: 2176... Loss: 5.1569... ppl: 45.2566  Val Loss: 3.8123\n",
      "Epoch: 1/10... Step: 2208... Loss: 4.1744... ppl: 45.2945  Val Loss: 3.8132\n",
      "Epoch: 1/10... Step: 2240... Loss: 3.6651... ppl: 45.2184  Val Loss: 3.8115\n",
      "Epoch: 1/10... Step: 2272... Loss: 3.9630... ppl: 44.3976  Val Loss: 3.7932\n",
      "Epoch: 1/10... Step: 2304... Loss: 3.2598... ppl: 44.0372  Val Loss: 3.7850\n",
      "Epoch: 1/10... Step: 2336... Loss: 4.2152... ppl: 43.8992  Val Loss: 3.7819\n",
      "Epoch: 2/10... Step: 2368... Loss: 3.1993... ppl: 43.9914  Val Loss: 3.7840\n",
      "Epoch: 2/10... Step: 2400... Loss: 3.4636... ppl: 43.9781  Val Loss: 3.7837\n",
      "Epoch: 2/10... Step: 2432... Loss: 3.8313... ppl: 44.0682  Val Loss: 3.7857\n",
      "Epoch: 2/10... Step: 2464... Loss: 4.1994... ppl: 43.7822  Val Loss: 3.7792\n",
      "Epoch: 2/10... Step: 2496... Loss: 3.9272... ppl: 43.3031  Val Loss: 3.7682\n",
      "Epoch: 2/10... Step: 2528... Loss: 3.5603... ppl: 43.1312  Val Loss: 3.7642\n",
      "Epoch: 2/10... Step: 2560... Loss: 5.2484... ppl: 42.9673  Val Loss: 3.7604\n",
      "Epoch: 2/10... Step: 2592... Loss: 4.0688... ppl: 43.0562  Val Loss: 3.7625\n",
      "Epoch: 2/10... Step: 2624... Loss: 3.1485... ppl: 42.8285  Val Loss: 3.7572\n",
      "Epoch: 2/10... Step: 2656... Loss: 3.8546... ppl: 42.5786  Val Loss: 3.7514\n",
      "Epoch: 2/10... Step: 2688... Loss: 3.3150... ppl: 42.3884  Val Loss: 3.7469\n",
      "Epoch: 2/10... Step: 2720... Loss: 3.7916... ppl: 41.9912  Val Loss: 3.7375\n",
      "Epoch: 2/10... Step: 2752... Loss: 3.5649... ppl: 42.0780  Val Loss: 3.7395\n",
      "Epoch: 2/10... Step: 2784... Loss: 4.2208... ppl: 41.9852  Val Loss: 3.7373\n",
      "Epoch: 2/10... Step: 2816... Loss: 4.0485... ppl: 42.0190  Val Loss: 3.7381\n",
      "Epoch: 2/10... Step: 2848... Loss: 3.6650... ppl: 41.6322  Val Loss: 3.7289\n",
      "Epoch: 2/10... Step: 2880... Loss: 4.1273... ppl: 41.6066  Val Loss: 3.7283\n",
      "Epoch: 2/10... Step: 2912... Loss: 3.2563... ppl: 41.2493  Val Loss: 3.7196\n",
      "Epoch: 2/10... Step: 2944... Loss: 3.3318... ppl: 41.3673  Val Loss: 3.7225\n",
      "Epoch: 2/10... Step: 2976... Loss: 3.3077... ppl: 40.9533  Val Loss: 3.7124\n",
      "Epoch: 2/10... Step: 3008... Loss: 4.2002... ppl: 41.0817  Val Loss: 3.7156\n",
      "Epoch: 2/10... Step: 3040... Loss: 3.5039... ppl: 40.7277  Val Loss: 3.7069\n",
      "Epoch: 2/10... Step: 3072... Loss: 3.6424... ppl: 40.5789  Val Loss: 3.7032\n",
      "Epoch: 2/10... Step: 3104... Loss: 3.7720... ppl: 40.5016  Val Loss: 3.7013\n",
      "Epoch: 2/10... Step: 3136... Loss: 4.0753... ppl: 40.2208  Val Loss: 3.6944\n",
      "Epoch: 2/10... Step: 3168... Loss: 3.4607... ppl: 40.2045  Val Loss: 3.6940\n",
      "Epoch: 2/10... Step: 3200... Loss: 3.7952... ppl: 40.4008  Val Loss: 3.6988\n",
      "Epoch: 2/10... Step: 3232... Loss: 4.2566... ppl: 40.0254  Val Loss: 3.6895\n",
      "Epoch: 2/10... Step: 3264... Loss: 2.8102... ppl: 39.6935  Val Loss: 3.6812\n",
      "Epoch: 2/10... Step: 3296... Loss: 4.2046... ppl: 39.7719  Val Loss: 3.6832\n",
      "Epoch: 2/10... Step: 3328... Loss: 4.1254... ppl: 39.6711  Val Loss: 3.6806\n",
      "Epoch: 2/10... Step: 3360... Loss: 3.7019... ppl: 39.2048  Val Loss: 3.6688\n",
      "Epoch: 2/10... Step: 3392... Loss: 3.1728... ppl: 39.5335  Val Loss: 3.6771\n",
      "Epoch: 2/10... Step: 3424... Loss: 4.3804... ppl: 39.2444  Val Loss: 3.6698\n",
      "Epoch: 2/10... Step: 3456... Loss: 3.4194... ppl: 39.0485  Val Loss: 3.6648\n",
      "Epoch: 2/10... Step: 3488... Loss: 3.6199... ppl: 39.1992  Val Loss: 3.6687\n",
      "Epoch: 2/10... Step: 3520... Loss: 3.4170... ppl: 38.8325  Val Loss: 3.6593\n",
      "Epoch: 2/10... Step: 3552... Loss: 4.0036... ppl: 38.9298  Val Loss: 3.6618\n",
      "Epoch: 2/10... Step: 3584... Loss: 2.9102... ppl: 38.6865  Val Loss: 3.6555\n",
      "Epoch: 2/10... Step: 3616... Loss: 3.5932... ppl: 38.4419  Val Loss: 3.6491\n",
      "Epoch: 2/10... Step: 3648... Loss: 3.2328... ppl: 38.6211  Val Loss: 3.6538\n",
      "Epoch: 2/10... Step: 3680... Loss: 3.6663... ppl: 38.5681  Val Loss: 3.6524\n",
      "Epoch: 2/10... Step: 3712... Loss: 3.4971... ppl: 38.3332  Val Loss: 3.6463\n",
      "Epoch: 2/10... Step: 3744... Loss: 3.4282... ppl: 38.1394  Val Loss: 3.6412\n",
      "Epoch: 2/10... Step: 3776... Loss: 4.0264... ppl: 38.2311  Val Loss: 3.6437\n",
      "Epoch: 2/10... Step: 3808... Loss: 4.9037... ppl: 38.0379  Val Loss: 3.6386\n",
      "Epoch: 2/10... Step: 3840... Loss: 4.2767... ppl: 38.2904  Val Loss: 3.6452\n",
      "Epoch: 2/10... Step: 3872... Loss: 3.9057... ppl: 37.9974  Val Loss: 3.6375\n",
      "Epoch: 2/10... Step: 3904... Loss: 3.0146... ppl: 37.9880  Val Loss: 3.6373\n",
      "Epoch: 2/10... Step: 3936... Loss: 3.9057... ppl: 37.9095  Val Loss: 3.6352\n",
      "Epoch: 2/10... Step: 3968... Loss: 3.9154... ppl: 37.5305  Val Loss: 3.6252\n",
      "Epoch: 2/10... Step: 4000... Loss: 3.6968... ppl: 37.5582  Val Loss: 3.6259\n",
      "Epoch: 2/10... Step: 4032... Loss: 4.1962... ppl: 37.4710  Val Loss: 3.6236\n",
      "Epoch: 2/10... Step: 4064... Loss: 3.6896... ppl: 37.4026  Val Loss: 3.6217\n",
      "Epoch: 2/10... Step: 4096... Loss: 3.5962... ppl: 37.4701  Val Loss: 3.6235\n",
      "Epoch: 2/10... Step: 4128... Loss: 2.8679... ppl: 37.4111  Val Loss: 3.6220\n",
      "Epoch: 2/10... Step: 4160... Loss: 2.8986... ppl: 37.4145  Val Loss: 3.6221\n",
      "Epoch: 2/10... Step: 4192... Loss: 3.4679... ppl: 37.0844  Val Loss: 3.6132\n",
      "Epoch: 2/10... Step: 4224... Loss: 3.3387... ppl: 36.6444  Val Loss: 3.6013\n",
      "Epoch: 2/10... Step: 4256... Loss: 3.5065... ppl: 36.8776  Val Loss: 3.6076\n",
      "Epoch: 2/10... Step: 4288... Loss: 3.4271... ppl: 36.5916  Val Loss: 3.5998\n",
      "Epoch: 2/10... Step: 4320... Loss: 2.8991... ppl: 36.6671  Val Loss: 3.6019\n",
      "Epoch: 2/10... Step: 4352... Loss: 3.0329... ppl: 36.5486  Val Loss: 3.5986\n",
      "Epoch: 2/10... Step: 4384... Loss: 3.2106... ppl: 36.4526  Val Loss: 3.5960\n",
      "Epoch: 2/10... Step: 4416... Loss: 4.0106... ppl: 36.4163  Val Loss: 3.5950\n",
      "Epoch: 2/10... Step: 4448... Loss: 2.9853... ppl: 36.4782  Val Loss: 3.5967\n",
      "Epoch: 2/10... Step: 4480... Loss: 4.3309... ppl: 36.3764  Val Loss: 3.5939\n",
      "Epoch: 2/10... Step: 4512... Loss: 3.2936... ppl: 36.2039  Val Loss: 3.5892\n",
      "Epoch: 2/10... Step: 4544... Loss: 3.5931... ppl: 36.1525  Val Loss: 3.5877\n",
      "Epoch: 2/10... Step: 4576... Loss: 2.8483... ppl: 36.2855  Val Loss: 3.5914\n",
      "Epoch: 2/10... Step: 4608... Loss: 2.9494... ppl: 36.1651  Val Loss: 3.5881\n",
      "Epoch: 2/10... Step: 4640... Loss: 3.6536... ppl: 35.9542  Val Loss: 3.5822\n",
      "Epoch: 2/10... Step: 4672... Loss: 3.5797... ppl: 35.9292  Val Loss: 3.5816\n",
      "Epoch: 3/10... Step: 4704... Loss: 3.1282... ppl: 35.8464  Val Loss: 3.5792\n",
      "Epoch: 3/10... Step: 4736... Loss: 4.1981... ppl: 36.0746  Val Loss: 3.5856\n",
      "Epoch: 3/10... Step: 4768... Loss: 3.0886... ppl: 36.1296  Val Loss: 3.5871\n",
      "Epoch: 3/10... Step: 4800... Loss: 2.6474... ppl: 36.2454  Val Loss: 3.5903\n",
      "Epoch: 3/10... Step: 4832... Loss: 3.3482... ppl: 36.0097  Val Loss: 3.5838\n",
      "Epoch: 3/10... Step: 4864... Loss: 3.7673... ppl: 35.8712  Val Loss: 3.5799\n",
      "Epoch: 3/10... Step: 4896... Loss: 2.4450... ppl: 35.9586  Val Loss: 3.5824\n",
      "Epoch: 3/10... Step: 4928... Loss: 2.1421... ppl: 35.8977  Val Loss: 3.5807\n",
      "Epoch: 3/10... Step: 4960... Loss: 3.1816... ppl: 35.8942  Val Loss: 3.5806\n",
      "Epoch: 3/10... Step: 4992... Loss: 3.4989... ppl: 36.3145  Val Loss: 3.5922\n",
      "Epoch: 3/10... Step: 5024... Loss: 3.4928... ppl: 35.8757  Val Loss: 3.5801\n",
      "Epoch: 3/10... Step: 5056... Loss: 3.0560... ppl: 35.6214  Val Loss: 3.5729\n",
      "Epoch: 3/10... Step: 5088... Loss: 2.8219... ppl: 35.6899  Val Loss: 3.5749\n",
      "Epoch: 3/10... Step: 5120... Loss: 3.7754... ppl: 35.6870  Val Loss: 3.5748\n",
      "Epoch: 3/10... Step: 5152... Loss: 2.9108... ppl: 35.7411  Val Loss: 3.5763\n",
      "Epoch: 3/10... Step: 5184... Loss: 2.9051... ppl: 35.7303  Val Loss: 3.5760\n",
      "Epoch: 3/10... Step: 5216... Loss: 2.9199... ppl: 35.4457  Val Loss: 3.5680\n",
      "Epoch: 3/10... Step: 5248... Loss: 3.1085... ppl: 35.3285  Val Loss: 3.5647\n",
      "Epoch: 3/10... Step: 5280... Loss: 3.2391... ppl: 35.5695  Val Loss: 3.5715\n",
      "Epoch: 3/10... Step: 5312... Loss: 3.5125... ppl: 35.3241  Val Loss: 3.5646\n",
      "Epoch: 3/10... Step: 5344... Loss: 3.5787... ppl: 35.4287  Val Loss: 3.5675\n",
      "Epoch: 3/10... Step: 5376... Loss: 4.0247... ppl: 35.3869  Val Loss: 3.5663\n",
      "Epoch: 3/10... Step: 5408... Loss: 3.7459... ppl: 35.2995  Val Loss: 3.5639\n",
      "Epoch: 3/10... Step: 5440... Loss: 4.3591... ppl: 35.4073  Val Loss: 3.5669\n",
      "Epoch: 3/10... Step: 5472... Loss: 3.7809... ppl: 35.0714  Val Loss: 3.5574\n",
      "Epoch: 3/10... Step: 5504... Loss: 3.6340... ppl: 35.2120  Val Loss: 3.5614\n",
      "Epoch: 3/10... Step: 5536... Loss: 3.2987... ppl: 35.4357  Val Loss: 3.5677\n",
      "Epoch: 3/10... Step: 5568... Loss: 3.0679... ppl: 35.1631  Val Loss: 3.5600\n",
      "Epoch: 3/10... Step: 5600... Loss: 3.4014... ppl: 35.1303  Val Loss: 3.5591\n",
      "Epoch: 3/10... Step: 5632... Loss: 3.3615... ppl: 35.1586  Val Loss: 3.5599\n",
      "Epoch: 3/10... Step: 5664... Loss: 3.1158... ppl: 35.1780  Val Loss: 3.5604\n",
      "Epoch: 3/10... Step: 5696... Loss: 3.7116... ppl: 34.8502  Val Loss: 3.5511\n",
      "Epoch: 3/10... Step: 5728... Loss: 3.2025... ppl: 35.1256  Val Loss: 3.5589\n",
      "Epoch: 3/10... Step: 5760... Loss: 3.8498... ppl: 34.9185  Val Loss: 3.5530\n",
      "Epoch: 3/10... Step: 5792... Loss: 3.9464... ppl: 34.8870  Val Loss: 3.5521\n",
      "Epoch: 3/10... Step: 5824... Loss: 3.1889... ppl: 34.9667  Val Loss: 3.5544\n",
      "Epoch: 3/10... Step: 5856... Loss: 3.4682... ppl: 34.6435  Val Loss: 3.5451\n",
      "Epoch: 3/10... Step: 5888... Loss: 3.0906... ppl: 34.7248  Val Loss: 3.5475\n",
      "Epoch: 3/10... Step: 5920... Loss: 3.5397... ppl: 34.5537  Val Loss: 3.5425\n",
      "Epoch: 3/10... Step: 5952... Loss: 3.7852... ppl: 34.4950  Val Loss: 3.5408\n",
      "Epoch: 3/10... Step: 5984... Loss: 3.3261... ppl: 34.6267  Val Loss: 3.5446\n",
      "Epoch: 3/10... Step: 6016... Loss: 3.8029... ppl: 34.5301  Val Loss: 3.5418\n",
      "Epoch: 3/10... Step: 6048... Loss: 3.3993... ppl: 34.7376  Val Loss: 3.5478\n",
      "Epoch: 3/10... Step: 6080... Loss: 2.7700... ppl: 34.4438  Val Loss: 3.5393\n",
      "Epoch: 3/10... Step: 6112... Loss: 3.4095... ppl: 34.4747  Val Loss: 3.5402\n",
      "Epoch: 3/10... Step: 6144... Loss: 3.1212... ppl: 34.3753  Val Loss: 3.5373\n",
      "Epoch: 3/10... Step: 6176... Loss: 3.1760... ppl: 34.4468  Val Loss: 3.5394\n",
      "Epoch: 3/10... Step: 6208... Loss: 3.4193... ppl: 34.5020  Val Loss: 3.5410\n",
      "Epoch: 3/10... Step: 6240... Loss: 2.7271... ppl: 34.5439  Val Loss: 3.5422\n",
      "Epoch: 3/10... Step: 6272... Loss: 2.6850... ppl: 34.5938  Val Loss: 3.5437\n",
      "Epoch: 3/10... Step: 6304... Loss: 3.0913... ppl: 34.3398  Val Loss: 3.5363\n",
      "Epoch: 3/10... Step: 6336... Loss: 3.4468... ppl: 34.2689  Val Loss: 3.5342\n",
      "Epoch: 3/10... Step: 6368... Loss: 3.1259... ppl: 34.3096  Val Loss: 3.5354\n",
      "Epoch: 3/10... Step: 6400... Loss: 3.6356... ppl: 34.3279  Val Loss: 3.5360\n",
      "Epoch: 3/10... Step: 6432... Loss: 3.5873... ppl: 34.3354  Val Loss: 3.5362\n",
      "Epoch: 3/10... Step: 6464... Loss: 3.5973... ppl: 34.3219  Val Loss: 3.5358\n",
      "Epoch: 3/10... Step: 6496... Loss: 2.7896... ppl: 34.2599  Val Loss: 3.5340\n",
      "Epoch: 3/10... Step: 6528... Loss: 3.0072... ppl: 34.1646  Val Loss: 3.5312\n",
      "Epoch: 3/10... Step: 6560... Loss: 3.3104... ppl: 34.0015  Val Loss: 3.5264\n",
      "Epoch: 3/10... Step: 6592... Loss: 2.5708... ppl: 34.0301  Val Loss: 3.5272\n",
      "Epoch: 3/10... Step: 6624... Loss: 3.6440... ppl: 33.7649  Val Loss: 3.5194\n",
      "Epoch: 3/10... Step: 6656... Loss: 3.0106... ppl: 33.8161  Val Loss: 3.5209\n",
      "Epoch: 3/10... Step: 6688... Loss: 2.8570... ppl: 34.0827  Val Loss: 3.5288\n",
      "Epoch: 3/10... Step: 6720... Loss: 2.5075... ppl: 33.8984  Val Loss: 3.5234\n",
      "Epoch: 3/10... Step: 6752... Loss: 3.6117... ppl: 33.7889  Val Loss: 3.5201\n",
      "Epoch: 3/10... Step: 6784... Loss: 3.2985... ppl: 33.9523  Val Loss: 3.5250\n",
      "Epoch: 3/10... Step: 6816... Loss: 2.7764... ppl: 33.9045  Val Loss: 3.5235\n",
      "Epoch: 3/10... Step: 6848... Loss: 2.8861... ppl: 33.6810  Val Loss: 3.5169\n",
      "Epoch: 3/10... Step: 6880... Loss: 3.0477... ppl: 33.7620  Val Loss: 3.5193\n",
      "Epoch: 3/10... Step: 6912... Loss: 4.1109... ppl: 33.9039  Val Loss: 3.5235\n",
      "Epoch: 3/10... Step: 6944... Loss: 3.1833... ppl: 34.0854  Val Loss: 3.5289\n",
      "Epoch: 3/10... Step: 6976... Loss: 3.4196... ppl: 33.8376  Val Loss: 3.5216\n",
      "Epoch: 3/10... Step: 7008... Loss: 3.4603... ppl: 33.7636  Val Loss: 3.5194\n",
      "Epoch: 4/10... Step: 7040... Loss: 3.5276... ppl: 33.6550  Val Loss: 3.5162\n",
      "Epoch: 4/10... Step: 7072... Loss: 2.9591... ppl: 33.8890  Val Loss: 3.5231\n",
      "Epoch: 4/10... Step: 7104... Loss: 3.2654... ppl: 33.7366  Val Loss: 3.5186\n",
      "Epoch: 4/10... Step: 7136... Loss: 2.8327... ppl: 33.9532  Val Loss: 3.5250\n",
      "Epoch: 4/10... Step: 7168... Loss: 3.3912... ppl: 33.9240  Val Loss: 3.5241\n",
      "Epoch: 4/10... Step: 7200... Loss: 2.7381... ppl: 33.7232  Val Loss: 3.5182\n",
      "Epoch: 4/10... Step: 7232... Loss: 2.9916... ppl: 33.9298  Val Loss: 3.5243\n",
      "Epoch: 4/10... Step: 7264... Loss: 4.1891... ppl: 33.8839  Val Loss: 3.5229\n",
      "Epoch: 4/10... Step: 7296... Loss: 3.6344... ppl: 33.9382  Val Loss: 3.5245\n",
      "Epoch: 4/10... Step: 7328... Loss: 2.5210... ppl: 34.3573  Val Loss: 3.5368\n",
      "Epoch: 4/10... Step: 7360... Loss: 2.8542... ppl: 34.0021  Val Loss: 3.5264\n",
      "Epoch: 4/10... Step: 7392... Loss: 3.0583... ppl: 33.8238  Val Loss: 3.5212\n",
      "Epoch: 4/10... Step: 7424... Loss: 2.7502... ppl: 33.6927  Val Loss: 3.5173\n",
      "Epoch: 4/10... Step: 7456... Loss: 2.7095... ppl: 33.8107  Val Loss: 3.5208\n",
      "Epoch: 4/10... Step: 7488... Loss: 3.4732... ppl: 33.8401  Val Loss: 3.5216\n",
      "Epoch: 4/10... Step: 7520... Loss: 2.0201... ppl: 34.0087  Val Loss: 3.5266\n",
      "Epoch: 4/10... Step: 7552... Loss: 3.2538... ppl: 33.8327  Val Loss: 3.5214\n",
      "Epoch: 4/10... Step: 7584... Loss: 2.5983... ppl: 33.7553  Val Loss: 3.5191\n",
      "Epoch: 4/10... Step: 7616... Loss: 2.9947... ppl: 33.8311  Val Loss: 3.5214\n",
      "Epoch: 4/10... Step: 7648... Loss: 3.4117... ppl: 33.7575  Val Loss: 3.5192\n",
      "Epoch: 4/10... Step: 7680... Loss: 2.8374... ppl: 33.7342  Val Loss: 3.5185\n",
      "Epoch: 4/10... Step: 7712... Loss: 2.7587... ppl: 33.8233  Val Loss: 3.5212\n",
      "Epoch: 4/10... Step: 7744... Loss: 3.3073... ppl: 33.8265  Val Loss: 3.5212\n",
      "Epoch: 4/10... Step: 7776... Loss: 3.7129... ppl: 33.8928  Val Loss: 3.5232\n",
      "Epoch: 4/10... Step: 7808... Loss: 3.5861... ppl: 33.6936  Val Loss: 3.5173\n",
      "Epoch: 4/10... Step: 7840... Loss: 2.1183... ppl: 33.7681  Val Loss: 3.5195\n",
      "Epoch: 4/10... Step: 7872... Loss: 3.8136... ppl: 33.9460  Val Loss: 3.5248\n",
      "Epoch: 4/10... Step: 7904... Loss: 2.2433... ppl: 33.8000  Val Loss: 3.5205\n",
      "Epoch: 4/10... Step: 7936... Loss: 3.1118... ppl: 33.8361  Val Loss: 3.5215\n",
      "Epoch: 4/10... Step: 7968... Loss: 3.7338... ppl: 33.6463  Val Loss: 3.5159\n",
      "Epoch: 4/10... Step: 8000... Loss: 3.0163... ppl: 33.9730  Val Loss: 3.5256\n",
      "Epoch: 4/10... Step: 8032... Loss: 3.3771... ppl: 33.7938  Val Loss: 3.5203\n",
      "Epoch: 4/10... Step: 8064... Loss: 2.5032... ppl: 33.8162  Val Loss: 3.5209\n",
      "Epoch: 4/10... Step: 8096... Loss: 3.0835... ppl: 33.7398  Val Loss: 3.5187\n",
      "Epoch: 4/10... Step: 8128... Loss: 2.2576... ppl: 33.7191  Val Loss: 3.5181\n",
      "Epoch: 4/10... Step: 8160... Loss: 2.8554... ppl: 33.7108  Val Loss: 3.5178\n",
      "Epoch: 4/10... Step: 8192... Loss: 3.2097... ppl: 33.5838  Val Loss: 3.5140\n",
      "Epoch: 4/10... Step: 8224... Loss: 3.0381... ppl: 33.5670  Val Loss: 3.5135\n",
      "Epoch: 4/10... Step: 8256... Loss: 2.6645... ppl: 33.4898  Val Loss: 3.5112\n",
      "Epoch: 4/10... Step: 8288... Loss: 3.1612... ppl: 33.4567  Val Loss: 3.5103\n",
      "Epoch: 4/10... Step: 8320... Loss: 3.1185... ppl: 33.3651  Val Loss: 3.5075\n",
      "Epoch: 4/10... Step: 8352... Loss: 2.7945... ppl: 33.3857  Val Loss: 3.5081\n",
      "Epoch: 4/10... Step: 8384... Loss: 2.6248... ppl: 33.7282  Val Loss: 3.5183\n",
      "Epoch: 4/10... Step: 8416... Loss: 2.7881... ppl: 33.4270  Val Loss: 3.5094\n",
      "Epoch: 4/10... Step: 8448... Loss: 4.1127... ppl: 33.4253  Val Loss: 3.5093\n",
      "Epoch: 4/10... Step: 8480... Loss: 2.7313... ppl: 33.4452  Val Loss: 3.5099\n",
      "Epoch: 4/10... Step: 8512... Loss: 2.4270... ppl: 33.4136  Val Loss: 3.5090\n",
      "Epoch: 4/10... Step: 8544... Loss: 2.9740... ppl: 33.5992  Val Loss: 3.5145\n",
      "Epoch: 4/10... Step: 8576... Loss: 2.8990... ppl: 33.5760  Val Loss: 3.5138\n",
      "Epoch: 4/10... Step: 8608... Loss: 3.3872... ppl: 33.5821  Val Loss: 3.5140\n",
      "Epoch: 4/10... Step: 8640... Loss: 3.2379... ppl: 33.5682  Val Loss: 3.5136\n",
      "Epoch: 4/10... Step: 8672... Loss: 2.6663... ppl: 33.3918  Val Loss: 3.5083\n",
      "Epoch: 4/10... Step: 8704... Loss: 3.6770... ppl: 33.3399  Val Loss: 3.5068\n",
      "Epoch: 4/10... Step: 8736... Loss: 2.9921... ppl: 33.5026  Val Loss: 3.5116\n",
      "Epoch: 4/10... Step: 8768... Loss: 2.7063... ppl: 33.4774  Val Loss: 3.5109\n",
      "Epoch: 4/10... Step: 8800... Loss: 3.4246... ppl: 33.5283  Val Loss: 3.5124\n",
      "Epoch: 4/10... Step: 8832... Loss: 2.9807... ppl: 33.3599  Val Loss: 3.5074\n",
      "Epoch: 4/10... Step: 8864... Loss: 3.2401... ppl: 33.5794  Val Loss: 3.5139\n",
      "Epoch: 4/10... Step: 8896... Loss: 2.7024... ppl: 33.3245  Val Loss: 3.5063\n",
      "Epoch: 4/10... Step: 8928... Loss: 2.7863... ppl: 33.3300  Val Loss: 3.5065\n",
      "Epoch: 4/10... Step: 8960... Loss: 2.4107... ppl: 33.0538  Val Loss: 3.4981\n",
      "Epoch: 4/10... Step: 8992... Loss: 3.0287... ppl: 33.0943  Val Loss: 3.4994\n",
      "Epoch: 4/10... Step: 9024... Loss: 3.4703... ppl: 33.4456  Val Loss: 3.5099\n",
      "Epoch: 4/10... Step: 9056... Loss: 3.0181... ppl: 33.2869  Val Loss: 3.5052\n",
      "Epoch: 4/10... Step: 9088... Loss: 3.4369... ppl: 33.2598  Val Loss: 3.5044\n",
      "Epoch: 4/10... Step: 9120... Loss: 2.6588... ppl: 33.3059  Val Loss: 3.5057\n",
      "Epoch: 4/10... Step: 9152... Loss: 2.9151... ppl: 33.3844  Val Loss: 3.5081\n",
      "Epoch: 4/10... Step: 9184... Loss: 3.3386... ppl: 33.1440  Val Loss: 3.5009\n",
      "Epoch: 4/10... Step: 9216... Loss: 2.3969... ppl: 33.2245  Val Loss: 3.5033\n",
      "Epoch: 4/10... Step: 9248... Loss: 3.6949... ppl: 33.2654  Val Loss: 3.5045\n",
      "Epoch: 4/10... Step: 9280... Loss: 2.6825... ppl: 33.8279  Val Loss: 3.5213\n",
      "Epoch: 4/10... Step: 9312... Loss: 3.3776... ppl: 33.3279  Val Loss: 3.5064\n",
      "Epoch: 4/10... Step: 9344... Loss: 3.1851... ppl: 33.2001  Val Loss: 3.5026\n",
      "Epoch: 5/10... Step: 9376... Loss: 1.9587... ppl: 33.2075  Val Loss: 3.5028\n",
      "Epoch: 5/10... Step: 9408... Loss: 3.2589... ppl: 33.4141  Val Loss: 3.5090\n",
      "Epoch: 5/10... Step: 9440... Loss: 2.9493... ppl: 33.1300  Val Loss: 3.5004\n",
      "Epoch: 5/10... Step: 9472... Loss: 2.7462... ppl: 33.3251  Val Loss: 3.5063\n",
      "Epoch: 5/10... Step: 9504... Loss: 3.0652... ppl: 33.4508  Val Loss: 3.5101\n",
      "Epoch: 5/10... Step: 9536... Loss: 2.1562... ppl: 33.2486  Val Loss: 3.5040\n",
      "Epoch: 5/10... Step: 9568... Loss: 1.8443... ppl: 33.5181  Val Loss: 3.5121\n",
      "Epoch: 5/10... Step: 9600... Loss: 3.0803... ppl: 33.4404  Val Loss: 3.5098\n",
      "Epoch: 5/10... Step: 9632... Loss: 3.0973... ppl: 33.5511  Val Loss: 3.5131\n",
      "Epoch: 5/10... Step: 9664... Loss: 3.1438... ppl: 33.6817  Val Loss: 3.5170\n",
      "Epoch: 5/10... Step: 9696... Loss: 2.2676... ppl: 33.7076  Val Loss: 3.5177\n",
      "Epoch: 5/10... Step: 9728... Loss: 3.1734... ppl: 33.4295  Val Loss: 3.5094\n",
      "Epoch: 5/10... Step: 9760... Loss: 2.4103... ppl: 33.1957  Val Loss: 3.5024\n",
      "Epoch: 5/10... Step: 9792... Loss: 3.4252... ppl: 33.3784  Val Loss: 3.5079\n",
      "Epoch: 5/10... Step: 9824... Loss: 3.2687... ppl: 33.3945  Val Loss: 3.5084\n",
      "Epoch: 5/10... Step: 9856... Loss: 2.0515... ppl: 33.6672  Val Loss: 3.5165\n",
      "Epoch: 5/10... Step: 9888... Loss: 3.1627... ppl: 33.5706  Val Loss: 3.5137\n",
      "Epoch: 5/10... Step: 9920... Loss: 2.9372... ppl: 33.6370  Val Loss: 3.5156\n",
      "Epoch: 5/10... Step: 9952... Loss: 3.2408... ppl: 33.4746  Val Loss: 3.5108\n",
      "Epoch: 5/10... Step: 9984... Loss: 3.1466... ppl: 33.5455  Val Loss: 3.5129\n",
      "Epoch: 5/10... Step: 10016... Loss: 2.7430... ppl: 33.5842  Val Loss: 3.5141\n",
      "Epoch: 5/10... Step: 10048... Loss: 2.7949... ppl: 33.6912  Val Loss: 3.5172\n",
      "Epoch: 5/10... Step: 10080... Loss: 2.9401... ppl: 33.5689  Val Loss: 3.5136\n",
      "Epoch: 5/10... Step: 10112... Loss: 3.7541... ppl: 33.6242  Val Loss: 3.5152\n",
      "Epoch: 5/10... Step: 10144... Loss: 2.7522... ppl: 33.5135  Val Loss: 3.5119\n",
      "Epoch: 5/10... Step: 10176... Loss: 2.4790... ppl: 33.6381  Val Loss: 3.5157\n",
      "Epoch: 5/10... Step: 10208... Loss: 3.2695... ppl: 33.6437  Val Loss: 3.5158\n",
      "Epoch: 5/10... Step: 10240... Loss: 3.0500... ppl: 33.6254  Val Loss: 3.5153\n",
      "Epoch: 5/10... Step: 10272... Loss: 3.5792... ppl: 33.6356  Val Loss: 3.5156\n",
      "Epoch: 5/10... Step: 10304... Loss: 3.6749... ppl: 33.4744  Val Loss: 3.5108\n",
      "Epoch: 5/10... Step: 10336... Loss: 2.9438... ppl: 33.7373  Val Loss: 3.5186\n",
      "Epoch: 5/10... Step: 10368... Loss: 2.8755... ppl: 33.7143  Val Loss: 3.5179\n",
      "Epoch: 5/10... Step: 10400... Loss: 2.8966... ppl: 33.5714  Val Loss: 3.5137\n",
      "Epoch: 5/10... Step: 10432... Loss: 2.9914... ppl: 33.7500  Val Loss: 3.5190\n",
      "Epoch: 5/10... Step: 10464... Loss: 2.9911... ppl: 33.6336  Val Loss: 3.5155\n",
      "Epoch: 5/10... Step: 10496... Loss: 2.6343... ppl: 33.6412  Val Loss: 3.5158\n",
      "Epoch: 5/10... Step: 10528... Loss: 3.1200... ppl: 33.6023  Val Loss: 3.5146\n",
      "Epoch: 5/10... Step: 10560... Loss: 3.5262... ppl: 33.4687  Val Loss: 3.5106\n",
      "Epoch: 5/10... Step: 10592... Loss: 2.9175... ppl: 33.4579  Val Loss: 3.5103\n",
      "Epoch: 5/10... Step: 10624... Loss: 2.8876... ppl: 33.5622  Val Loss: 3.5134\n",
      "Epoch: 5/10... Step: 10656... Loss: 2.9491... ppl: 33.3279  Val Loss: 3.5064\n",
      "Epoch: 5/10... Step: 10688... Loss: 2.7661... ppl: 33.4042  Val Loss: 3.5087\n",
      "Epoch: 5/10... Step: 10720... Loss: 2.8069... ppl: 33.6596  Val Loss: 3.5163\n",
      "Epoch: 5/10... Step: 10752... Loss: 2.6450... ppl: 33.5378  Val Loss: 3.5127\n",
      "Epoch: 5/10... Step: 10784... Loss: 3.5406... ppl: 33.3762  Val Loss: 3.5078\n",
      "Epoch: 5/10... Step: 10816... Loss: 3.0399... ppl: 33.5541  Val Loss: 3.5132\n",
      "Epoch: 5/10... Step: 10848... Loss: 3.4037... ppl: 33.4371  Val Loss: 3.5097\n",
      "Epoch: 5/10... Step: 10880... Loss: 2.7487... ppl: 33.5630  Val Loss: 3.5134\n",
      "Epoch: 5/10... Step: 10912... Loss: 2.3871... ppl: 33.5731  Val Loss: 3.5137\n",
      "Epoch: 5/10... Step: 10944... Loss: 3.1419... ppl: 33.6451  Val Loss: 3.5159\n",
      "Epoch: 5/10... Step: 10976... Loss: 3.1780... ppl: 33.6512  Val Loss: 3.5160\n",
      "Epoch: 5/10... Step: 11008... Loss: 3.2067... ppl: 33.5407  Val Loss: 3.5128\n",
      "Epoch: 5/10... Step: 11040... Loss: 3.5821... ppl: 33.4354  Val Loss: 3.5096\n",
      "Epoch: 5/10... Step: 11072... Loss: 3.5248... ppl: 33.6377  Val Loss: 3.5156\n",
      "Epoch: 5/10... Step: 11104... Loss: 3.0730... ppl: 33.5059  Val Loss: 3.5117\n",
      "Epoch: 5/10... Step: 11136... Loss: 3.6976... ppl: 33.5271  Val Loss: 3.5124\n",
      "Epoch: 5/10... Step: 11168... Loss: 2.4382... ppl: 33.4981  Val Loss: 3.5115\n",
      "Epoch: 5/10... Step: 11200... Loss: 2.9397... ppl: 33.6278  Val Loss: 3.5154\n",
      "Epoch: 5/10... Step: 11232... Loss: 2.6212... ppl: 33.4281  Val Loss: 3.5094\n",
      "Epoch: 5/10... Step: 11264... Loss: 2.1745... ppl: 33.3967  Val Loss: 3.5085\n",
      "Epoch: 5/10... Step: 11296... Loss: 2.7252... ppl: 33.3398  Val Loss: 3.5068\n",
      "Epoch: 5/10... Step: 11328... Loss: 3.5518... ppl: 33.1824  Val Loss: 3.5020\n",
      "Epoch: 5/10... Step: 11360... Loss: 2.7291... ppl: 33.5201  Val Loss: 3.5121\n",
      "Epoch: 5/10... Step: 11392... Loss: 2.6292... ppl: 33.4453  Val Loss: 3.5099\n",
      "Epoch: 5/10... Step: 11424... Loss: 3.4973... ppl: 33.3702  Val Loss: 3.5077\n",
      "Epoch: 5/10... Step: 11456... Loss: 2.3156... ppl: 33.4334  Val Loss: 3.5096\n",
      "Epoch: 5/10... Step: 11488... Loss: 2.8402... ppl: 33.5594  Val Loss: 3.5133\n",
      "Epoch: 5/10... Step: 11520... Loss: 2.3132... ppl: 33.3462  Val Loss: 3.5069\n",
      "Epoch: 5/10... Step: 11552... Loss: 3.8121... ppl: 33.3221  Val Loss: 3.5062\n",
      "Epoch: 5/10... Step: 11584... Loss: 2.9044... ppl: 33.3721  Val Loss: 3.5077\n",
      "Epoch: 5/10... Step: 11616... Loss: 2.3553... ppl: 33.9292  Val Loss: 3.5243\n",
      "Epoch: 5/10... Step: 11648... Loss: 2.8168... ppl: 33.6291  Val Loss: 3.5154\n",
      "Epoch: 5/10... Step: 11680... Loss: 3.2964... ppl: 33.4737  Val Loss: 3.5108\n",
      "Epoch: 5/10... Step: 11712... Loss: 3.0574... ppl: 33.4669  Val Loss: 3.5106\n",
      "Epoch: 6/10... Step: 11744... Loss: 3.4900... ppl: 33.5698  Val Loss: 3.5136\n",
      "Epoch: 6/10... Step: 11776... Loss: 2.9208... ppl: 33.4594  Val Loss: 3.5103\n",
      "Epoch: 6/10... Step: 11808... Loss: 2.6116... ppl: 33.4826  Val Loss: 3.5110\n",
      "Epoch: 6/10... Step: 11840... Loss: 3.3777... ppl: 33.6023  Val Loss: 3.5146\n",
      "Epoch: 6/10... Step: 11872... Loss: 2.8791... ppl: 33.4350  Val Loss: 3.5096\n",
      "Epoch: 6/10... Step: 11904... Loss: 3.1072... ppl: 33.6626  Val Loss: 3.5164\n",
      "Epoch: 6/10... Step: 11936... Loss: 2.9789... ppl: 33.7051  Val Loss: 3.5177\n",
      "Epoch: 6/10... Step: 11968... Loss: 2.7108... ppl: 33.8114  Val Loss: 3.5208\n",
      "Epoch: 6/10... Step: 12000... Loss: 2.4765... ppl: 33.7465  Val Loss: 3.5189\n",
      "Epoch: 6/10... Step: 12032... Loss: 2.9191... ppl: 34.2030  Val Loss: 3.5323\n",
      "Epoch: 6/10... Step: 12064... Loss: 2.9733... ppl: 33.7548  Val Loss: 3.5191\n",
      "Epoch: 6/10... Step: 12096... Loss: 3.8216... ppl: 33.5145  Val Loss: 3.5120\n",
      "Epoch: 6/10... Step: 12128... Loss: 2.5310... ppl: 33.6087  Val Loss: 3.5148\n",
      "Epoch: 6/10... Step: 12160... Loss: 3.0516... ppl: 33.8103  Val Loss: 3.5208\n",
      "Epoch: 6/10... Step: 12192... Loss: 2.8269... ppl: 33.9877  Val Loss: 3.5260\n",
      "Epoch: 6/10... Step: 12224... Loss: 2.5925... ppl: 33.9323  Val Loss: 3.5244\n",
      "Epoch: 6/10... Step: 12256... Loss: 2.6673... ppl: 34.0225  Val Loss: 3.5270\n",
      "Epoch: 6/10... Step: 12288... Loss: 2.7954... ppl: 33.8235  Val Loss: 3.5212\n",
      "Epoch: 6/10... Step: 12320... Loss: 2.8199... ppl: 33.9724  Val Loss: 3.5255\n",
      "Epoch: 6/10... Step: 12352... Loss: 2.6504... ppl: 34.0398  Val Loss: 3.5275\n",
      "Epoch: 6/10... Step: 12384... Loss: 2.9923... ppl: 34.0370  Val Loss: 3.5274\n",
      "Epoch: 6/10... Step: 12416... Loss: 3.0551... ppl: 34.0158  Val Loss: 3.5268\n",
      "Epoch: 6/10... Step: 12448... Loss: 2.9802... ppl: 33.8945  Val Loss: 3.5233\n",
      "Epoch: 6/10... Step: 12480... Loss: 2.6271... ppl: 33.9279  Val Loss: 3.5242\n",
      "Epoch: 6/10... Step: 12512... Loss: 2.7817... ppl: 33.9085  Val Loss: 3.5237\n",
      "Epoch: 6/10... Step: 12544... Loss: 2.8196... ppl: 34.1434  Val Loss: 3.5306\n",
      "Epoch: 6/10... Step: 12576... Loss: 2.4867... ppl: 34.1741  Val Loss: 3.5315\n",
      "Epoch: 6/10... Step: 12608... Loss: 3.1516... ppl: 34.0389  Val Loss: 3.5275\n",
      "Epoch: 6/10... Step: 12640... Loss: 2.5483... ppl: 33.9130  Val Loss: 3.5238\n",
      "Epoch: 6/10... Step: 12672... Loss: 3.1097... ppl: 34.0577  Val Loss: 3.5281\n",
      "Epoch: 6/10... Step: 12704... Loss: 2.5348... ppl: 34.1317  Val Loss: 3.5302\n",
      "Epoch: 6/10... Step: 12736... Loss: 3.1925... ppl: 33.9136  Val Loss: 3.5238\n",
      "Epoch: 6/10... Step: 12768... Loss: 2.5695... ppl: 34.2047  Val Loss: 3.5324\n",
      "Epoch: 6/10... Step: 12800... Loss: 2.9167... ppl: 34.1107  Val Loss: 3.5296\n",
      "Epoch: 6/10... Step: 12832... Loss: 2.9383... ppl: 34.1298  Val Loss: 3.5302\n",
      "Epoch: 6/10... Step: 12864... Loss: 2.8955... ppl: 34.2102  Val Loss: 3.5325\n",
      "Epoch: 6/10... Step: 12896... Loss: 3.3703... ppl: 33.9153  Val Loss: 3.5239\n",
      "Epoch: 6/10... Step: 12928... Loss: 2.7153... ppl: 33.9172  Val Loss: 3.5239\n",
      "Epoch: 6/10... Step: 12960... Loss: 2.9627... ppl: 34.0028  Val Loss: 3.5264\n",
      "Epoch: 6/10... Step: 12992... Loss: 2.9920... ppl: 33.8833  Val Loss: 3.5229\n",
      "Epoch: 6/10... Step: 13024... Loss: 2.3881... ppl: 33.9956  Val Loss: 3.5262\n",
      "Epoch: 6/10... Step: 13056... Loss: 2.6987... ppl: 34.0889  Val Loss: 3.5290\n",
      "Epoch: 6/10... Step: 13088... Loss: 2.6970... ppl: 34.2554  Val Loss: 3.5338\n",
      "Epoch: 6/10... Step: 13120... Loss: 3.2103... ppl: 33.9794  Val Loss: 3.5258\n",
      "Epoch: 6/10... Step: 13152... Loss: 2.8061... ppl: 34.0382  Val Loss: 3.5275\n",
      "Epoch: 6/10... Step: 13184... Loss: 3.3505... ppl: 33.9488  Val Loss: 3.5249\n",
      "Epoch: 6/10... Step: 13216... Loss: 3.4938... ppl: 33.9539  Val Loss: 3.5250\n",
      "Epoch: 6/10... Step: 13248... Loss: 3.2787... ppl: 34.0606  Val Loss: 3.5281\n",
      "Epoch: 6/10... Step: 13280... Loss: 3.1299... ppl: 34.0874  Val Loss: 3.5289\n",
      "Epoch: 6/10... Step: 13312... Loss: 2.8703... ppl: 34.2110  Val Loss: 3.5325\n",
      "Epoch: 6/10... Step: 13344... Loss: 2.3599... ppl: 34.0777  Val Loss: 3.5286\n",
      "Epoch: 6/10... Step: 13376... Loss: 2.5472... ppl: 34.0301  Val Loss: 3.5272\n",
      "Epoch: 6/10... Step: 13408... Loss: 2.1645... ppl: 34.2789  Val Loss: 3.5345\n",
      "Epoch: 6/10... Step: 13440... Loss: 3.3531... ppl: 34.1688  Val Loss: 3.5313\n",
      "Epoch: 6/10... Step: 13472... Loss: 3.1304... ppl: 34.2466  Val Loss: 3.5336\n",
      "Epoch: 6/10... Step: 13504... Loss: 2.8122... ppl: 34.3184  Val Loss: 3.5357\n",
      "Epoch: 6/10... Step: 13536... Loss: 3.1615... ppl: 34.2861  Val Loss: 3.5347\n",
      "Epoch: 6/10... Step: 13568... Loss: 2.7239... ppl: 34.1282  Val Loss: 3.5301\n",
      "Epoch: 6/10... Step: 13600... Loss: 2.8142... ppl: 34.0007  Val Loss: 3.5264\n",
      "Epoch: 6/10... Step: 13632... Loss: 3.4637... ppl: 34.2644  Val Loss: 3.5341\n",
      "Epoch: 6/10... Step: 13664... Loss: 2.7844... ppl: 33.8890  Val Loss: 3.5231\n",
      "Epoch: 6/10... Step: 13696... Loss: 3.1403... ppl: 33.9730  Val Loss: 3.5256\n",
      "Epoch: 6/10... Step: 13728... Loss: 2.4052... ppl: 34.1088  Val Loss: 3.5296\n",
      "Epoch: 6/10... Step: 13760... Loss: 3.1383... ppl: 33.9769  Val Loss: 3.5257\n",
      "Epoch: 6/10... Step: 13792... Loss: 3.0815... ppl: 33.9954  Val Loss: 3.5262\n",
      "Epoch: 6/10... Step: 13824... Loss: 2.5110... ppl: 34.2072  Val Loss: 3.5324\n",
      "Epoch: 6/10... Step: 13856... Loss: 3.1638... ppl: 34.1025  Val Loss: 3.5294\n",
      "Epoch: 6/10... Step: 13888... Loss: 2.7815... ppl: 33.8731  Val Loss: 3.5226\n",
      "Epoch: 6/10... Step: 13920... Loss: 3.3347... ppl: 34.0443  Val Loss: 3.5277\n",
      "Epoch: 6/10... Step: 13952... Loss: 2.2250... ppl: 34.3519  Val Loss: 3.5367\n",
      "Epoch: 6/10... Step: 13984... Loss: 3.0183... ppl: 34.4701  Val Loss: 3.5401\n",
      "Epoch: 6/10... Step: 14016... Loss: 2.5356... ppl: 34.1647  Val Loss: 3.5312\n",
      "Epoch: 6/10... Step: 14048... Loss: 2.5271... ppl: 34.0407  Val Loss: 3.5276\n",
      "Epoch: 7/10... Step: 14080... Loss: 3.0539... ppl: 34.1376  Val Loss: 3.5304\n",
      "Epoch: 7/10... Step: 14112... Loss: 2.0840... ppl: 34.2185  Val Loss: 3.5328\n",
      "Epoch: 7/10... Step: 14144... Loss: 2.7210... ppl: 34.0116  Val Loss: 3.5267\n",
      "Epoch: 7/10... Step: 14176... Loss: 2.7809... ppl: 34.2323  Val Loss: 3.5332\n",
      "Epoch: 7/10... Step: 14208... Loss: 2.4533... ppl: 34.1503  Val Loss: 3.5308\n",
      "Epoch: 7/10... Step: 14240... Loss: 2.7728... ppl: 34.1930  Val Loss: 3.5320\n",
      "Epoch: 7/10... Step: 14272... Loss: 2.3852... ppl: 34.3390  Val Loss: 3.5363\n",
      "Epoch: 7/10... Step: 14304... Loss: 2.3178... ppl: 34.4086  Val Loss: 3.5383\n",
      "Epoch: 7/10... Step: 14336... Loss: 2.7147... ppl: 34.3749  Val Loss: 3.5373\n",
      "Epoch: 7/10... Step: 14368... Loss: 3.4594... ppl: 35.0126  Val Loss: 3.5557\n",
      "Epoch: 7/10... Step: 14400... Loss: 2.6315... ppl: 34.4815  Val Loss: 3.5404\n",
      "Epoch: 7/10... Step: 14432... Loss: 2.9139... ppl: 34.2209  Val Loss: 3.5328\n",
      "Epoch: 7/10... Step: 14464... Loss: 3.1593... ppl: 34.1698  Val Loss: 3.5313\n",
      "Epoch: 7/10... Step: 14496... Loss: 3.1265... ppl: 34.4700  Val Loss: 3.5401\n",
      "Epoch: 7/10... Step: 14528... Loss: 2.1156... ppl: 34.6442  Val Loss: 3.5451\n",
      "Epoch: 7/10... Step: 14560... Loss: 2.4218... ppl: 34.7131  Val Loss: 3.5471\n",
      "Epoch: 7/10... Step: 14592... Loss: 2.4918... ppl: 34.5886  Val Loss: 3.5435\n",
      "Epoch: 7/10... Step: 14624... Loss: 2.1103... ppl: 34.5926  Val Loss: 3.5436\n",
      "Epoch: 7/10... Step: 14656... Loss: 2.9875... ppl: 34.7535  Val Loss: 3.5483\n",
      "Epoch: 7/10... Step: 14688... Loss: 2.9830... ppl: 34.7470  Val Loss: 3.5481\n",
      "Epoch: 7/10... Step: 14720... Loss: 3.6982... ppl: 34.8054  Val Loss: 3.5498\n",
      "Epoch: 7/10... Step: 14752... Loss: 2.8519... ppl: 34.8321  Val Loss: 3.5505\n",
      "Epoch: 7/10... Step: 14784... Loss: 2.1321... ppl: 34.7407  Val Loss: 3.5479\n",
      "Epoch: 7/10... Step: 14816... Loss: 2.6809... ppl: 34.7702  Val Loss: 3.5488\n",
      "Epoch: 7/10... Step: 14848... Loss: 2.5898... ppl: 34.5801  Val Loss: 3.5433\n",
      "Epoch: 7/10... Step: 14880... Loss: 2.7610... ppl: 34.8796  Val Loss: 3.5519\n",
      "Epoch: 7/10... Step: 14912... Loss: 2.5967... ppl: 35.1010  Val Loss: 3.5582\n",
      "Epoch: 7/10... Step: 14944... Loss: 2.9766... ppl: 34.9054  Val Loss: 3.5526\n",
      "Epoch: 7/10... Step: 14976... Loss: 2.6451... ppl: 34.7738  Val Loss: 3.5489\n",
      "Epoch: 7/10... Step: 15008... Loss: 2.7688... ppl: 34.8708  Val Loss: 3.5516\n",
      "Epoch: 7/10... Step: 15040... Loss: 2.1364... ppl: 34.9162  Val Loss: 3.5529\n",
      "Epoch: 7/10... Step: 15072... Loss: 3.1049... ppl: 34.8353  Val Loss: 3.5506\n",
      "Epoch: 7/10... Step: 15104... Loss: 2.8140... ppl: 35.0775  Val Loss: 3.5576\n",
      "Epoch: 7/10... Step: 15136... Loss: 3.2189... ppl: 34.9851  Val Loss: 3.5549\n",
      "Epoch: 7/10... Step: 15168... Loss: 2.4227... ppl: 34.9432  Val Loss: 3.5537\n",
      "Epoch: 7/10... Step: 15200... Loss: 2.6966... ppl: 34.9788  Val Loss: 3.5547\n",
      "Epoch: 7/10... Step: 15232... Loss: 3.1124... ppl: 34.7519  Val Loss: 3.5482\n",
      "Epoch: 7/10... Step: 15264... Loss: 2.7333... ppl: 34.7930  Val Loss: 3.5494\n",
      "Epoch: 7/10... Step: 15296... Loss: 2.9519... ppl: 34.6779  Val Loss: 3.5461\n",
      "Epoch: 7/10... Step: 15328... Loss: 2.6552... ppl: 34.7469  Val Loss: 3.5481\n",
      "Epoch: 7/10... Step: 15360... Loss: 2.7439... ppl: 34.8806  Val Loss: 3.5519\n",
      "Epoch: 7/10... Step: 15392... Loss: 2.7957... ppl: 34.7708  Val Loss: 3.5488\n",
      "Epoch: 7/10... Step: 15424... Loss: 2.2360... ppl: 35.0686  Val Loss: 3.5573\n",
      "Epoch: 7/10... Step: 15456... Loss: 3.0941... ppl: 34.8056  Val Loss: 3.5498\n",
      "Epoch: 7/10... Step: 15488... Loss: 2.8335... ppl: 34.8084  Val Loss: 3.5499\n",
      "Epoch: 7/10... Step: 15520... Loss: 2.9576... ppl: 34.7910  Val Loss: 3.5494\n",
      "Epoch: 7/10... Step: 15552... Loss: 2.6061... ppl: 34.6294  Val Loss: 3.5447\n",
      "Epoch: 7/10... Step: 15584... Loss: 3.2872... ppl: 34.8556  Val Loss: 3.5512\n",
      "Epoch: 7/10... Step: 15616... Loss: 2.1625... ppl: 34.9607  Val Loss: 3.5542\n",
      "Epoch: 7/10... Step: 15648... Loss: 2.8190... ppl: 35.0967  Val Loss: 3.5581\n",
      "Epoch: 7/10... Step: 15680... Loss: 2.6265... ppl: 35.1195  Val Loss: 3.5588\n",
      "Epoch: 7/10... Step: 15712... Loss: 2.1093... ppl: 35.0736  Val Loss: 3.5574\n",
      "Epoch: 7/10... Step: 15744... Loss: 2.7668... ppl: 35.1017  Val Loss: 3.5582\n",
      "Epoch: 7/10... Step: 15776... Loss: 2.9313... ppl: 35.0279  Val Loss: 3.5561\n",
      "Epoch: 7/10... Step: 15808... Loss: 3.0921... ppl: 35.0883  Val Loss: 3.5579\n",
      "Epoch: 7/10... Step: 15840... Loss: 3.5155... ppl: 35.1650  Val Loss: 3.5601\n",
      "Epoch: 7/10... Step: 15872... Loss: 3.1972... ppl: 34.8619  Val Loss: 3.5514\n",
      "Epoch: 7/10... Step: 15904... Loss: 2.4287... ppl: 35.0669  Val Loss: 3.5573\n",
      "Epoch: 7/10... Step: 15936... Loss: 2.6517... ppl: 35.0067  Val Loss: 3.5555\n",
      "Epoch: 7/10... Step: 15968... Loss: 2.0009... ppl: 35.1057  Val Loss: 3.5584\n",
      "Epoch: 7/10... Step: 16000... Loss: 2.8322... ppl: 34.7764  Val Loss: 3.5489\n",
      "Epoch: 7/10... Step: 16032... Loss: 2.5811... ppl: 34.7563  Val Loss: 3.5484\n",
      "Epoch: 7/10... Step: 16064... Loss: 2.2991... ppl: 35.1123  Val Loss: 3.5586\n",
      "Epoch: 7/10... Step: 16096... Loss: 2.2467... ppl: 34.8477  Val Loss: 3.5510\n",
      "Epoch: 7/10... Step: 16128... Loss: 2.4931... ppl: 34.8229  Val Loss: 3.5503\n",
      "Epoch: 7/10... Step: 16160... Loss: 2.7458... ppl: 35.0544  Val Loss: 3.5569\n",
      "Epoch: 7/10... Step: 16192... Loss: 2.6052... ppl: 35.1066  Val Loss: 3.5584\n",
      "Epoch: 7/10... Step: 16224... Loss: 3.1930... ppl: 34.6694  Val Loss: 3.5459\n",
      "Epoch: 7/10... Step: 16256... Loss: 2.4540... ppl: 34.8015  Val Loss: 3.5497\n",
      "Epoch: 7/10... Step: 16288... Loss: 2.7941... ppl: 35.0118  Val Loss: 3.5557\n",
      "Epoch: 7/10... Step: 16320... Loss: 2.3627... ppl: 35.4771  Val Loss: 3.5689\n",
      "Epoch: 7/10... Step: 16352... Loss: 2.5739... ppl: 35.1986  Val Loss: 3.5610\n",
      "Epoch: 7/10... Step: 16384... Loss: 3.3484... ppl: 34.9411  Val Loss: 3.5537\n",
      "Epoch: 8/10... Step: 16416... Loss: 3.1771... ppl: 34.9358  Val Loss: 3.5535\n",
      "Epoch: 8/10... Step: 16448... Loss: 2.6957... ppl: 35.1673  Val Loss: 3.5601\n",
      "Epoch: 8/10... Step: 16480... Loss: 2.6911... ppl: 34.7607  Val Loss: 3.5485\n",
      "Epoch: 8/10... Step: 16512... Loss: 1.7422... ppl: 34.9488  Val Loss: 3.5539\n",
      "Epoch: 8/10... Step: 16544... Loss: 2.6871... ppl: 34.9561  Val Loss: 3.5541\n",
      "Epoch: 8/10... Step: 16576... Loss: 2.8377... ppl: 34.9206  Val Loss: 3.5531\n",
      "Epoch: 8/10... Step: 16608... Loss: 2.8394... ppl: 35.1980  Val Loss: 3.5610\n",
      "Epoch: 8/10... Step: 16640... Loss: 2.4700... ppl: 35.1410  Val Loss: 3.5594\n",
      "Epoch: 8/10... Step: 16672... Loss: 2.7770... ppl: 35.1101  Val Loss: 3.5585\n",
      "Epoch: 8/10... Step: 16704... Loss: 2.6639... ppl: 35.7210  Val Loss: 3.5757\n",
      "Epoch: 8/10... Step: 16736... Loss: 2.9551... ppl: 35.5831  Val Loss: 3.5719\n",
      "Epoch: 8/10... Step: 16768... Loss: 2.9003... ppl: 35.1886  Val Loss: 3.5607\n",
      "Epoch: 8/10... Step: 16800... Loss: 2.2662... ppl: 34.9945  Val Loss: 3.5552\n",
      "Epoch: 8/10... Step: 16832... Loss: 2.3850... ppl: 35.2104  Val Loss: 3.5613\n",
      "Epoch: 8/10... Step: 16864... Loss: 2.6803... ppl: 35.4344  Val Loss: 3.5677\n",
      "Epoch: 8/10... Step: 16896... Loss: 2.3122... ppl: 35.5824  Val Loss: 3.5719\n",
      "Epoch: 8/10... Step: 16928... Loss: 2.4720... ppl: 35.3748  Val Loss: 3.5660\n",
      "Epoch: 8/10... Step: 16960... Loss: 2.4725... ppl: 35.5188  Val Loss: 3.5701\n",
      "Epoch: 8/10... Step: 16992... Loss: 2.6779... ppl: 35.6150  Val Loss: 3.5728\n",
      "Epoch: 8/10... Step: 17024... Loss: 2.1776... ppl: 35.5278  Val Loss: 3.5703\n",
      "Epoch: 8/10... Step: 17056... Loss: 2.5102... ppl: 35.6757  Val Loss: 3.5745\n",
      "Epoch: 8/10... Step: 17088... Loss: 3.2696... ppl: 35.7539  Val Loss: 3.5767\n",
      "Epoch: 8/10... Step: 17120... Loss: 2.6253... ppl: 35.6564  Val Loss: 3.5739\n",
      "Epoch: 8/10... Step: 17152... Loss: 3.0103... ppl: 35.6758  Val Loss: 3.5745\n",
      "Epoch: 8/10... Step: 17184... Loss: 2.6440... ppl: 35.4699  Val Loss: 3.5687\n",
      "Epoch: 8/10... Step: 17216... Loss: 2.3279... ppl: 35.7581  Val Loss: 3.5768\n",
      "Epoch: 8/10... Step: 17248... Loss: 2.3350... ppl: 36.0002  Val Loss: 3.5835\n",
      "Epoch: 8/10... Step: 17280... Loss: 2.3468... ppl: 35.8066  Val Loss: 3.5781\n",
      "Epoch: 8/10... Step: 17312... Loss: 3.4533... ppl: 35.6570  Val Loss: 3.5739\n",
      "Epoch: 8/10... Step: 17344... Loss: 2.6267... ppl: 35.6812  Val Loss: 3.5746\n",
      "Epoch: 8/10... Step: 17376... Loss: 2.6840... ppl: 35.8201  Val Loss: 3.5785\n",
      "Epoch: 8/10... Step: 17408... Loss: 2.6015... ppl: 35.7569  Val Loss: 3.5767\n",
      "Epoch: 8/10... Step: 17440... Loss: 2.6076... ppl: 35.9222  Val Loss: 3.5814\n",
      "Epoch: 8/10... Step: 17472... Loss: 2.6741... ppl: 36.0030  Val Loss: 3.5836\n",
      "Epoch: 8/10... Step: 17504... Loss: 2.6431... ppl: 35.9609  Val Loss: 3.5824\n",
      "Epoch: 8/10... Step: 17536... Loss: 2.7847... ppl: 36.0716  Val Loss: 3.5855\n",
      "Epoch: 8/10... Step: 17568... Loss: 2.7544... ppl: 35.8771  Val Loss: 3.5801\n",
      "Epoch: 8/10... Step: 17600... Loss: 2.6831... ppl: 35.8851  Val Loss: 3.5803\n",
      "Epoch: 8/10... Step: 17632... Loss: 2.5592... ppl: 35.6296  Val Loss: 3.5732\n",
      "Epoch: 8/10... Step: 17664... Loss: 2.5266... ppl: 35.8077  Val Loss: 3.5782\n",
      "Epoch: 8/10... Step: 17696... Loss: 3.0039... ppl: 35.7338  Val Loss: 3.5761\n",
      "Epoch: 8/10... Step: 17728... Loss: 3.0299... ppl: 35.6934  Val Loss: 3.5750\n",
      "Epoch: 8/10... Step: 17760... Loss: 2.6363... ppl: 36.0512  Val Loss: 3.5849\n",
      "Epoch: 8/10... Step: 17792... Loss: 2.4559... ppl: 35.9050  Val Loss: 3.5809\n",
      "Epoch: 8/10... Step: 17824... Loss: 2.6598... ppl: 35.8378  Val Loss: 3.5790\n",
      "Epoch: 8/10... Step: 17856... Loss: 2.3363... ppl: 35.7574  Val Loss: 3.5768\n",
      "Epoch: 8/10... Step: 17888... Loss: 2.8807... ppl: 35.6794  Val Loss: 3.5746\n",
      "Epoch: 8/10... Step: 17920... Loss: 2.2327... ppl: 35.7069  Val Loss: 3.5753\n",
      "Epoch: 8/10... Step: 17952... Loss: 2.7268... ppl: 35.8482  Val Loss: 3.5793\n",
      "Epoch: 8/10... Step: 17984... Loss: 2.6597... ppl: 36.0749  Val Loss: 3.5856\n",
      "Epoch: 8/10... Step: 18016... Loss: 2.7498... ppl: 36.0211  Val Loss: 3.5841\n",
      "Epoch: 8/10... Step: 18048... Loss: 3.1201... ppl: 35.8887  Val Loss: 3.5804\n",
      "Epoch: 8/10... Step: 18080... Loss: 2.9185... ppl: 35.9949  Val Loss: 3.5834\n",
      "Epoch: 8/10... Step: 18112... Loss: 2.2464... ppl: 35.9669  Val Loss: 3.5826\n",
      "Epoch: 8/10... Step: 18144... Loss: 3.4118... ppl: 35.9583  Val Loss: 3.5824\n",
      "Epoch: 8/10... Step: 18176... Loss: 2.7394... ppl: 35.9850  Val Loss: 3.5831\n",
      "Epoch: 8/10... Step: 18208... Loss: 2.7017... ppl: 35.8619  Val Loss: 3.5797\n",
      "Epoch: 8/10... Step: 18240... Loss: 2.8097... ppl: 36.0561  Val Loss: 3.5851\n",
      "Epoch: 8/10... Step: 18272... Loss: 2.0434... ppl: 36.0059  Val Loss: 3.5837\n",
      "Epoch: 8/10... Step: 18304... Loss: 2.8273... ppl: 35.9792  Val Loss: 3.5829\n",
      "Epoch: 8/10... Step: 18336... Loss: 2.9458... ppl: 35.7421  Val Loss: 3.5763\n",
      "Epoch: 8/10... Step: 18368... Loss: 2.9054... ppl: 35.6128  Val Loss: 3.5727\n",
      "Epoch: 8/10... Step: 18400... Loss: 2.7148... ppl: 36.1505  Val Loss: 3.5877\n",
      "Epoch: 8/10... Step: 18432... Loss: 3.1131... ppl: 35.8446  Val Loss: 3.5792\n",
      "Epoch: 8/10... Step: 18464... Loss: 2.4474... ppl: 35.6532  Val Loss: 3.5738\n",
      "Epoch: 8/10... Step: 18496... Loss: 2.1801... ppl: 35.9200  Val Loss: 3.5813\n",
      "Epoch: 8/10... Step: 18528... Loss: 2.9760... ppl: 36.0263  Val Loss: 3.5842\n",
      "Epoch: 8/10... Step: 18560... Loss: 2.4788... ppl: 35.6766  Val Loss: 3.5745\n",
      "Epoch: 8/10... Step: 18592... Loss: 2.7811... ppl: 35.7788  Val Loss: 3.5774\n",
      "Epoch: 8/10... Step: 18624... Loss: 2.8533... ppl: 35.9076  Val Loss: 3.5809\n",
      "Epoch: 8/10... Step: 18656... Loss: 3.6625... ppl: 36.5821  Val Loss: 3.5996\n",
      "Epoch: 8/10... Step: 18688... Loss: 2.2616... ppl: 36.3368  Val Loss: 3.5928\n",
      "Epoch: 8/10... Step: 18720... Loss: 2.5581... ppl: 35.9479  Val Loss: 3.5821\n",
      "Epoch: 9/10... Step: 18752... Loss: 2.4759... ppl: 35.8929  Val Loss: 3.5805\n",
      "Epoch: 9/10... Step: 18784... Loss: 2.5906... ppl: 36.2413  Val Loss: 3.5902\n",
      "Epoch: 9/10... Step: 18816... Loss: 2.8316... ppl: 35.8183  Val Loss: 3.5785\n",
      "Epoch: 9/10... Step: 18848... Loss: 2.8349... ppl: 35.8163  Val Loss: 3.5784\n",
      "Epoch: 9/10... Step: 18880... Loss: 2.8734... ppl: 36.0722  Val Loss: 3.5855\n",
      "Epoch: 9/10... Step: 18912... Loss: 2.8675... ppl: 35.9112  Val Loss: 3.5810\n",
      "Epoch: 9/10... Step: 18944... Loss: 2.7089... ppl: 36.1843  Val Loss: 3.5886\n",
      "Epoch: 9/10... Step: 18976... Loss: 2.7349... ppl: 36.2246  Val Loss: 3.5897\n",
      "Epoch: 9/10... Step: 19008... Loss: 2.8140... ppl: 36.1263  Val Loss: 3.5870\n",
      "Epoch: 9/10... Step: 19040... Loss: 2.3922... ppl: 36.5061  Val Loss: 3.5975\n",
      "Epoch: 9/10... Step: 19072... Loss: 2.5380... ppl: 36.6876  Val Loss: 3.6024\n",
      "Epoch: 9/10... Step: 19104... Loss: 2.5820... ppl: 36.2488  Val Loss: 3.5904\n",
      "Epoch: 9/10... Step: 19136... Loss: 1.8899... ppl: 35.9247  Val Loss: 3.5814\n",
      "Epoch: 9/10... Step: 19168... Loss: 2.3168... ppl: 35.9954  Val Loss: 3.5834\n",
      "Epoch: 9/10... Step: 19200... Loss: 3.0065... ppl: 36.3307  Val Loss: 3.5927\n",
      "Epoch: 9/10... Step: 19232... Loss: 2.8843... ppl: 36.5920  Val Loss: 3.5998\n",
      "Epoch: 9/10... Step: 19264... Loss: 2.8482... ppl: 36.4415  Val Loss: 3.5957\n",
      "Epoch: 9/10... Step: 19296... Loss: 3.1553... ppl: 36.5180  Val Loss: 3.5978\n",
      "Epoch: 9/10... Step: 19328... Loss: 2.8784... ppl: 36.5912  Val Loss: 3.5998\n",
      "Epoch: 9/10... Step: 19360... Loss: 3.0259... ppl: 36.5545  Val Loss: 3.5988\n",
      "Epoch: 9/10... Step: 19392... Loss: 2.4737... ppl: 36.6890  Val Loss: 3.6025\n",
      "Epoch: 9/10... Step: 19424... Loss: 2.7841... ppl: 36.6806  Val Loss: 3.6022\n",
      "Epoch: 9/10... Step: 19456... Loss: 2.6197... ppl: 36.6480  Val Loss: 3.6014\n",
      "Epoch: 9/10... Step: 19488... Loss: 2.6117... ppl: 36.7104  Val Loss: 3.6031\n",
      "Epoch: 9/10... Step: 19520... Loss: 2.7158... ppl: 36.5239  Val Loss: 3.5980\n",
      "Epoch: 9/10... Step: 19552... Loss: 2.2278... ppl: 36.6800  Val Loss: 3.6022\n",
      "Epoch: 9/10... Step: 19584... Loss: 2.2820... ppl: 36.9940  Val Loss: 3.6108\n",
      "Epoch: 9/10... Step: 19616... Loss: 2.8251... ppl: 36.8616  Val Loss: 3.6072\n",
      "Epoch: 9/10... Step: 19648... Loss: 2.7973... ppl: 36.6505  Val Loss: 3.6014\n",
      "Epoch: 9/10... Step: 19680... Loss: 2.6093... ppl: 36.6019  Val Loss: 3.6001\n",
      "Epoch: 9/10... Step: 19712... Loss: 2.2773... ppl: 36.7703  Val Loss: 3.6047\n",
      "Epoch: 9/10... Step: 19744... Loss: 1.9686... ppl: 36.7893  Val Loss: 3.6052\n",
      "Epoch: 9/10... Step: 19776... Loss: 2.9946... ppl: 36.8038  Val Loss: 3.6056\n",
      "Epoch: 9/10... Step: 19808... Loss: 2.0735... ppl: 36.9786  Val Loss: 3.6103\n",
      "Epoch: 9/10... Step: 19840... Loss: 2.2977... ppl: 36.9089  Val Loss: 3.6085\n",
      "Epoch: 9/10... Step: 19872... Loss: 2.8211... ppl: 36.9803  Val Loss: 3.6104\n",
      "Epoch: 9/10... Step: 19904... Loss: 2.5159... ppl: 36.9078  Val Loss: 3.6084\n",
      "Epoch: 9/10... Step: 19936... Loss: 2.4958... ppl: 36.7490  Val Loss: 3.6041\n",
      "Epoch: 9/10... Step: 19968... Loss: 2.5619... ppl: 36.5724  Val Loss: 3.5993\n",
      "Epoch: 9/10... Step: 20000... Loss: 3.0224... ppl: 36.6524  Val Loss: 3.6015\n",
      "Epoch: 9/10... Step: 20032... Loss: 2.9218... ppl: 36.6881  Val Loss: 3.6025\n",
      "Epoch: 9/10... Step: 20064... Loss: 2.5572... ppl: 36.6010  Val Loss: 3.6001\n",
      "Epoch: 9/10... Step: 20096... Loss: 2.5422... ppl: 37.0169  Val Loss: 3.6114\n",
      "Epoch: 9/10... Step: 20128... Loss: 2.6990... ppl: 37.0013  Val Loss: 3.6110\n",
      "Epoch: 9/10... Step: 20160... Loss: 2.7661... ppl: 37.0035  Val Loss: 3.6110\n",
      "Epoch: 9/10... Step: 20192... Loss: 2.3042... ppl: 36.9702  Val Loss: 3.6101\n",
      "Epoch: 9/10... Step: 20224... Loss: 2.4634... ppl: 36.7840  Val Loss: 3.6051\n",
      "Epoch: 9/10... Step: 20256... Loss: 2.4475... ppl: 36.6849  Val Loss: 3.6024\n",
      "Epoch: 9/10... Step: 20288... Loss: 2.8798... ppl: 36.9268  Val Loss: 3.6089\n",
      "Epoch: 9/10... Step: 20320... Loss: 2.6534... ppl: 37.1078  Val Loss: 3.6138\n",
      "Epoch: 9/10... Step: 20352... Loss: 2.3916... ppl: 37.0292  Val Loss: 3.6117\n",
      "Epoch: 9/10... Step: 20384... Loss: 2.8424... ppl: 36.9803  Val Loss: 3.6104\n",
      "Epoch: 9/10... Step: 20416... Loss: 2.6536... ppl: 37.0003  Val Loss: 3.6109\n",
      "Epoch: 9/10... Step: 20448... Loss: 2.5177... ppl: 37.1107  Val Loss: 3.6139\n",
      "Epoch: 9/10... Step: 20480... Loss: 2.5081... ppl: 37.0403  Val Loss: 3.6120\n",
      "Epoch: 9/10... Step: 20512... Loss: 2.8130... ppl: 37.0340  Val Loss: 3.6118\n",
      "Epoch: 9/10... Step: 20544... Loss: 2.6426... ppl: 36.9341  Val Loss: 3.6091\n",
      "Epoch: 9/10... Step: 20576... Loss: 2.5736... ppl: 37.1147  Val Loss: 3.6140\n",
      "Epoch: 9/10... Step: 20608... Loss: 2.4645... ppl: 37.0092  Val Loss: 3.6112\n",
      "Epoch: 9/10... Step: 20640... Loss: 2.2485... ppl: 37.1167  Val Loss: 3.6141\n",
      "Epoch: 9/10... Step: 20672... Loss: 2.6810... ppl: 36.9744  Val Loss: 3.6102\n",
      "Epoch: 9/10... Step: 20704... Loss: 2.8102... ppl: 36.5726  Val Loss: 3.5993\n",
      "Epoch: 9/10... Step: 20736... Loss: 2.5046... ppl: 36.9541  Val Loss: 3.6097\n",
      "Epoch: 9/10... Step: 20768... Loss: 2.8662... ppl: 36.9938  Val Loss: 3.6108\n",
      "Epoch: 9/10... Step: 20800... Loss: 2.8125... ppl: 36.8648  Val Loss: 3.6073\n",
      "Epoch: 9/10... Step: 20832... Loss: 2.8551... ppl: 36.9849  Val Loss: 3.6105\n",
      "Epoch: 9/10... Step: 20864... Loss: 2.3962... ppl: 37.0855  Val Loss: 3.6132\n",
      "Epoch: 9/10... Step: 20896... Loss: 2.7379... ppl: 36.8566  Val Loss: 3.6070\n",
      "Epoch: 9/10... Step: 20928... Loss: 2.4799... ppl: 36.7398  Val Loss: 3.6039\n",
      "Epoch: 9/10... Step: 20960... Loss: 2.5549... ppl: 36.8859  Val Loss: 3.6078\n",
      "Epoch: 9/10... Step: 20992... Loss: 2.2498... ppl: 37.6271  Val Loss: 3.6277\n",
      "Epoch: 9/10... Step: 21024... Loss: 2.2581... ppl: 37.4631  Val Loss: 3.6234\n",
      "Epoch: 9/10... Step: 21056... Loss: 2.6025... ppl: 37.0356  Val Loss: 3.6119\n",
      "Epoch: 10/10... Step: 21088... Loss: 2.7189... ppl: 36.9061  Val Loss: 3.6084\n",
      "Epoch: 10/10... Step: 21120... Loss: 2.4960... ppl: 37.1044  Val Loss: 3.6137\n",
      "Epoch: 10/10... Step: 21152... Loss: 2.6013... ppl: 36.9839  Val Loss: 3.6105\n",
      "Epoch: 10/10... Step: 21184... Loss: 2.6824... ppl: 36.9170  Val Loss: 3.6087\n",
      "Epoch: 10/10... Step: 21216... Loss: 2.2778... ppl: 37.0837  Val Loss: 3.6132\n",
      "Epoch: 10/10... Step: 21248... Loss: 2.4414... ppl: 36.9498  Val Loss: 3.6096\n",
      "Epoch: 10/10... Step: 21280... Loss: 3.2110... ppl: 37.2127  Val Loss: 3.6166\n",
      "Epoch: 10/10... Step: 21312... Loss: 2.7149... ppl: 37.2126  Val Loss: 3.6166\n",
      "Epoch: 10/10... Step: 21344... Loss: 2.6020... ppl: 37.2422  Val Loss: 3.6174\n",
      "Epoch: 10/10... Step: 21376... Loss: 2.3926... ppl: 37.2444  Val Loss: 3.6175\n",
      "Epoch: 10/10... Step: 21408... Loss: 2.6485... ppl: 37.7860  Val Loss: 3.6319\n",
      "Epoch: 10/10... Step: 21440... Loss: 2.6749... ppl: 37.4674  Val Loss: 3.6235\n",
      "Epoch: 10/10... Step: 21472... Loss: 2.5527... ppl: 37.0576  Val Loss: 3.6125\n",
      "Epoch: 10/10... Step: 21504... Loss: 2.5580... ppl: 37.1760  Val Loss: 3.6157\n",
      "Epoch: 10/10... Step: 21536... Loss: 2.4643... ppl: 37.4155  Val Loss: 3.6221\n",
      "Epoch: 10/10... Step: 21568... Loss: 2.6807... ppl: 37.7128  Val Loss: 3.6300\n",
      "Epoch: 10/10... Step: 21600... Loss: 2.3007... ppl: 37.5765  Val Loss: 3.6264\n",
      "Epoch: 10/10... Step: 21632... Loss: 2.4188... ppl: 37.6431  Val Loss: 3.6281\n",
      "Epoch: 10/10... Step: 21664... Loss: 2.6084... ppl: 37.5728  Val Loss: 3.6263\n",
      "Epoch: 10/10... Step: 21696... Loss: 2.9178... ppl: 37.7234  Val Loss: 3.6303\n",
      "Epoch: 10/10... Step: 21728... Loss: 2.4411... ppl: 37.8334  Val Loss: 3.6332\n",
      "Epoch: 10/10... Step: 21760... Loss: 2.1801... ppl: 37.7516  Val Loss: 3.6310\n",
      "Epoch: 10/10... Step: 21792... Loss: 2.2980... ppl: 37.7791  Val Loss: 3.6318\n",
      "Epoch: 10/10... Step: 21824... Loss: 2.4954... ppl: 37.8041  Val Loss: 3.6324\n",
      "Epoch: 10/10... Step: 21856... Loss: 2.5419... ppl: 37.7312  Val Loss: 3.6305\n",
      "Epoch: 10/10... Step: 21888... Loss: 2.3544... ppl: 37.8462  Val Loss: 3.6335\n",
      "Epoch: 10/10... Step: 21920... Loss: 2.8429... ppl: 38.0586  Val Loss: 3.6391\n",
      "Epoch: 10/10... Step: 21952... Loss: 2.8081... ppl: 38.1746  Val Loss: 3.6422\n",
      "Epoch: 10/10... Step: 21984... Loss: 2.0283... ppl: 37.8706  Val Loss: 3.6342\n",
      "Epoch: 10/10... Step: 22016... Loss: 2.5581... ppl: 37.7175  Val Loss: 3.6301\n",
      "Epoch: 10/10... Step: 22048... Loss: 2.1916... ppl: 37.9123  Val Loss: 3.6353\n",
      "Epoch: 10/10... Step: 22080... Loss: 2.7274... ppl: 37.8943  Val Loss: 3.6348\n",
      "Epoch: 10/10... Step: 22112... Loss: 2.1078... ppl: 37.8975  Val Loss: 3.6349\n",
      "Epoch: 10/10... Step: 22144... Loss: 2.3751... ppl: 38.0967  Val Loss: 3.6401\n",
      "Epoch: 10/10... Step: 22176... Loss: 2.3152... ppl: 38.1193  Val Loss: 3.6407\n",
      "Epoch: 10/10... Step: 22208... Loss: 2.7502... ppl: 38.0915  Val Loss: 3.6400\n",
      "Epoch: 10/10... Step: 22240... Loss: 2.5084... ppl: 38.0976  Val Loss: 3.6402\n",
      "Epoch: 10/10... Step: 22272... Loss: 2.6373... ppl: 37.7233  Val Loss: 3.6303\n",
      "Epoch: 10/10... Step: 22304... Loss: 2.7503... ppl: 37.7367  Val Loss: 3.6306\n",
      "Epoch: 10/10... Step: 22336... Loss: 2.5293... ppl: 37.7781  Val Loss: 3.6317\n",
      "Epoch: 10/10... Step: 22368... Loss: 2.9696... ppl: 37.7862  Val Loss: 3.6319\n",
      "Epoch: 10/10... Step: 22400... Loss: 2.4230... ppl: 37.6674  Val Loss: 3.6288\n",
      "Epoch: 10/10... Step: 22432... Loss: 2.4204... ppl: 37.9274  Val Loss: 3.6357\n",
      "Epoch: 10/10... Step: 22464... Loss: 1.8153... ppl: 38.1636  Val Loss: 3.6419\n",
      "Epoch: 10/10... Step: 22496... Loss: 2.3236... ppl: 38.0610  Val Loss: 3.6392\n",
      "Epoch: 10/10... Step: 22528... Loss: 2.8793... ppl: 38.1663  Val Loss: 3.6420\n",
      "Epoch: 10/10... Step: 22560... Loss: 2.5034... ppl: 37.8968  Val Loss: 3.6349\n",
      "Epoch: 10/10... Step: 22592... Loss: 2.2434... ppl: 37.7248  Val Loss: 3.6303\n",
      "Epoch: 10/10... Step: 22624... Loss: 2.6859... ppl: 37.9712  Val Loss: 3.6368\n",
      "Epoch: 10/10... Step: 22656... Loss: 2.3666... ppl: 38.3048  Val Loss: 3.6456\n",
      "Epoch: 10/10... Step: 22688... Loss: 2.8845... ppl: 38.2208  Val Loss: 3.6434\n",
      "Epoch: 10/10... Step: 22720... Loss: 2.2869... ppl: 38.1117  Val Loss: 3.6405\n",
      "Epoch: 10/10... Step: 22752... Loss: 2.7741... ppl: 38.1421  Val Loss: 3.6413\n",
      "Epoch: 10/10... Step: 22784... Loss: 2.7661... ppl: 38.3726  Val Loss: 3.6473\n",
      "Epoch: 10/10... Step: 22816... Loss: 2.5562... ppl: 38.1316  Val Loss: 3.6410\n",
      "Epoch: 10/10... Step: 22848... Loss: 2.8011... ppl: 38.1271  Val Loss: 3.6409\n",
      "Epoch: 10/10... Step: 22880... Loss: 2.2522... ppl: 38.2432  Val Loss: 3.6440\n",
      "Epoch: 10/10... Step: 22912... Loss: 1.9652... ppl: 38.2934  Val Loss: 3.6453\n",
      "Epoch: 10/10... Step: 22944... Loss: 2.8336... ppl: 38.2822  Val Loss: 3.6450\n",
      "Epoch: 10/10... Step: 22976... Loss: 2.4714... ppl: 38.2821  Val Loss: 3.6450\n",
      "Epoch: 10/10... Step: 23008... Loss: 2.6586... ppl: 38.3175  Val Loss: 3.6459\n",
      "Epoch: 10/10... Step: 23040... Loss: 2.2202... ppl: 37.8229  Val Loss: 3.6329\n",
      "Epoch: 10/10... Step: 23072... Loss: 2.5268... ppl: 38.0022  Val Loss: 3.6376\n",
      "Epoch: 10/10... Step: 23104... Loss: 2.5632... ppl: 38.2889  Val Loss: 3.6452\n",
      "Epoch: 10/10... Step: 23136... Loss: 2.4916... ppl: 38.0705  Val Loss: 3.6394\n",
      "Epoch: 10/10... Step: 23168... Loss: 2.4839... ppl: 37.9860  Val Loss: 3.6372\n",
      "Epoch: 10/10... Step: 23200... Loss: 2.2270... ppl: 38.1824  Val Loss: 3.6424\n",
      "Epoch: 10/10... Step: 23232... Loss: 2.4769... ppl: 38.1220  Val Loss: 3.6408\n",
      "Epoch: 10/10... Step: 23264... Loss: 2.4867... ppl: 37.8552  Val Loss: 3.6338\n",
      "Epoch: 10/10... Step: 23296... Loss: 2.4721... ppl: 37.9305  Val Loss: 3.6358\n",
      "Epoch: 10/10... Step: 23328... Loss: 2.4662... ppl: 38.4937  Val Loss: 3.6505\n",
      "Epoch: 10/10... Step: 23360... Loss: 2.9581... ppl: 38.7354  Val Loss: 3.6568\n",
      "Epoch: 10/10... Step: 23392... Loss: 2.5827... ppl: 38.3056  Val Loss: 3.6456\n",
      "Epoch: 10/10... Step: 23424... Loss: 2.1308... ppl: 38.0151  Val Loss: 3.6380\n"
     ]
    }
   ],
   "source": [
    "# specify batch size\n",
    "batch_size = 64\n",
    "\n",
    "# train the model\n",
    "train(net, batch_size = batch_size, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rQ6leiAm0Wmn"
   },
   "source": [
    "# 6. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1396,
     "status": "ok",
     "timestamp": 1596874248978,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "FPoIBPQpgqZy",
    "outputId": "8ee728df-b6cb-4bb9-88cb-c7c4524fce89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 76,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "net.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1279,
     "status": "ok",
     "timestamp": 1596874485946,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "bch2a0WglQ96"
   },
   "outputs": [],
   "source": [
    "# function to generate one token\n",
    "def predict(net, tkn, h=None):\n",
    "         \n",
    "  # tensor inputs\n",
    "  x = np.array([[token2int[tkn]]])\n",
    "  inputs = torch.from_numpy(x)\n",
    "  \n",
    "  if(torch.cuda.is_available()):\n",
    "      inputs = inputs.cuda()\n",
    "\n",
    "  # get the output of the model\n",
    "  out, h = net(inputs, h)\n",
    "\n",
    "  # get the token probabilities\n",
    "  p = F.softmax(out, dim=1).data\n",
    "\n",
    "  if(torch.cuda.is_available()):\n",
    "      p = p.cpu()\n",
    "\n",
    "  p = p.numpy()\n",
    "  sampled_token_index = np.argmax(p, axis = 1)[0]\n",
    "  \n",
    "  # return the encoded value of the predicted char and the hidden state\n",
    "  return int2token[sampled_token_index], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1290,
     "status": "ok",
     "timestamp": 1596874664194,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "EQR-euTiFan9"
   },
   "outputs": [],
   "source": [
    "# function to fetch generated sequence\n",
    "def sample(net, size = 2, seed_text='it is'):\n",
    "        \n",
    "    if(torch.cuda.is_available()):\n",
    "        net.cuda()\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    h = net.init_hidden(1)\n",
    "\n",
    "    toks = seed_text.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in toks:\n",
    "      token, h = predict(net, t, h)\n",
    "    \n",
    "    toks.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(size-1):\n",
    "        token, h = predict(net, toks[-1], h)\n",
    "        toks.append(token)\n",
    "\n",
    "    return ' '.join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1286,
     "status": "ok",
     "timestamp": 1596874758920,
     "user": {
      "displayName": "Prateek Joshi",
      "photoUrl": "",
      "userId": "14172408186104425556"
     },
     "user_tz": -330
    },
    "id": "VSakRw3SHRv2",
    "outputId": "a15311db-a4b8-4d17-a949-4e5e3df8036a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed text: i want to >> output: i want to order a pizza from pizza hut\n",
      "\n",
      "\n",
      "seed text: how about a cup >> output: how about a cup of coffee to pick up from\n",
      "\n",
      "\n",
      "seed text: i don't want >> output: i don't want to drive it in the morning\n",
      "\n",
      "\n",
      "seed text: can you send >> output: can you send me the confirmation to the office\n",
      "\n",
      "\n",
      "seed text: my car >> output: my car is making a weird noise when\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# seed texts\n",
    "seeds = [\"i want to\",\n",
    "         \"how about a cup\",\n",
    "         \"i don't want\",\n",
    "         \"can you send\",\n",
    "         \"my car\"]\n",
    "\n",
    "# number of tokens to generate\n",
    "num_toks = 6\n",
    "\n",
    "# text generation\n",
    "for s in seeds:\n",
    "  # get generated text from the model\n",
    "  text_gen = sample(net, num_toks, seed_text=s)\n",
    "  # print the result\n",
    "  print(\"seed text:\", s, \">> output:\",text_gen)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text Generation using Neural Language Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1eccf91efb3641ab93437844fb3d8515": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "233b3deec9394d288ed235c2054abe08": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "433348a8133446d0a9a4dc38f028bdc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "92d7ebd42a4e463eaed43f3960cfe36c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b00386afebe447d989c1d40fc75ebb24",
       "IPY_MODEL_bb3bafbb85f14bd0a9904c4f08d40946"
      ],
      "layout": "IPY_MODEL_1eccf91efb3641ab93437844fb3d8515"
     }
    },
    "b00386afebe447d989c1d40fc75ebb24": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c684ec2cc5e9413a822c072387aeedd5",
      "max": 64776,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_433348a8133446d0a9a4dc38f028bdc6",
      "value": 64776
     }
    },
    "bb3bafbb85f14bd0a9904c4f08d40946": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbb68ce201524cfa8eafec85788f9c42",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_233b3deec9394d288ed235c2054abe08",
      "value": " 64776/64776 [00:18&lt;00:00, 3464.93it/s]"
     }
    },
    "c684ec2cc5e9413a822c072387aeedd5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cbb68ce201524cfa8eafec85788f9c42": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
